{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe01c33-6027-4f26-8b6a-589c327e4ba2",
   "metadata": {},
   "source": [
    "# Making game setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f06cca4-f68e-4208-aeb3-a815e5fb44cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "\n",
    "from config_3_4_2 import config_3_4_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7184dbe8-9c27-431b-a31c-73b2c8660eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeds\n",
    "SEED = 100\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d80a3c40-5785-4c2d-a2a0-17b97b5c7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Board:\n",
    "    def __init__(self, flasks, color, max_length, empty=2):\n",
    "        self.color = color\n",
    "        self.max_length = max_length\n",
    "        self.empty = empty\n",
    "        self.tubes = self.color + self.empty\n",
    "        self.flasks = np.full((self.tubes, self.max_length), -1, dtype=np.int16)\n",
    "        self.flasks[:self.color, :self.max_length] = flasks[:-empty]\n",
    "        self.idx = np.zeros(self.tubes, dtype=np.uint16)\n",
    "        self.idx[:self.color] = self.max_length\n",
    "        self.actions = []\n",
    "        self.states = set()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.flasks)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.flasks)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.flasks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.flasks)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.flasks[item]\n",
    "\n",
    "    def grid_to_state(self):\n",
    "        state = []\n",
    "        for r in self:\n",
    "            state.append([])\n",
    "            for c in r:\n",
    "                if c != -1:\n",
    "                    state[-1].append(c)\n",
    "        return state\n",
    "\n",
    "    def state_to_grid(self, state):\n",
    "        self.flasks.fill(-1)\n",
    "        self.idx.fill(0)\n",
    "        for i, r in enumerate(state):\n",
    "            self.idx[i] = len(r)\n",
    "            for j, c in enumerate(r):\n",
    "                self.flasks[i][j] = c\n",
    "\n",
    "    def is_flask_full(self, i):\n",
    "        return self.idx[i] == self.max_length\n",
    "\n",
    "    def is_flask_empty(self, i):\n",
    "        return self.idx[i] == 0\n",
    "\n",
    "    def has_one_color(self, i):\n",
    "        return (self.flasks[i] == self.flasks[i][0]).all()\n",
    "\n",
    "    def is_flask_solved(self, i):\n",
    "        if self.is_flask_empty(i) or (self.is_flask_full(i) and self.has_one_color(i)):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def top(self, i):\n",
    "        return self.flasks[i][self.idx[i]-1]\n",
    "\n",
    "    def pop(self, i):\n",
    "        ball = self.flasks[i][self.idx[i]-1]\n",
    "        self.flasks[i][self.idx[i] - 1] = -1\n",
    "        self.idx[i] -= 1\n",
    "        return ball\n",
    "\n",
    "    def push(self, i, ball):\n",
    "        self.flasks[i][self.idx[i]] = ball\n",
    "        self.idx[i] += 1\n",
    "\n",
    "    def is_push_allowed(self, i, ball):\n",
    "        if self.is_flask_empty(i) or (self.top(i) == ball and not self.is_flask_full(i)):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_invalid_init_state(self):\n",
    "        return any(self.is_flask_full(i) and self.has_one_color(i) for i in range(self.tubes))\n",
    "\n",
    "    def is_solved(self):\n",
    "        return all(self.is_flask_solved(i) for i in range(self.tubes))\n",
    "\n",
    "    def is_deadend(self):\n",
    "        return not self.is_solved() and not self.valid_actions()\n",
    "\n",
    "    def reward(self):\n",
    "        if self.is_solved():\n",
    "            return 1\n",
    "        if self.is_deadend():\n",
    "            return -1\n",
    "        return 0\n",
    "\n",
    "    def valid_actions(self):\n",
    "        actions = []\n",
    "        for i in range(self.tubes):\n",
    "            if self.is_flask_solved(i):\n",
    "                continue\n",
    "            if self.is_flask_empty(i):\n",
    "                continue\n",
    "            top_i = self.top(i)\n",
    "            for j in range(self.tubes):\n",
    "                if i != j and self.is_push_allowed(j, top_i):\n",
    "                    actions.append((i, j))\n",
    "        return actions\n",
    "\n",
    "    def play(self, action):\n",
    "        self.push(action[1], self.pop(action[0]))\n",
    "        self.actions.append(action)\n",
    "\n",
    "    def undo_action(self):\n",
    "        action = self.actions.pop()\n",
    "        self.push(action[0], self.pop(action[1]))\n",
    "\n",
    "    def dfs_solve(self, steps=0, path=[]):\n",
    "        if self.is_solved():\n",
    "            return True, steps, path\n",
    "        if self.is_deadend():\n",
    "            print(\"hey\")\n",
    "            return False, steps, path\n",
    "        for a in self.valid_actions():\n",
    "            self.play(a)\n",
    "            current_state = str(self)\n",
    "            if current_state in self.states:\n",
    "                self.undo_action()\n",
    "                continue\n",
    "            self.states.add(current_state)\n",
    "            path.append(deepcopy(self))\n",
    "            r = self.dfs_solve(steps+1)\n",
    "            if r[0]:\n",
    "                return r\n",
    "            self.undo_action()\n",
    "        return False, steps, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "419f3dc4-f74d-42f3-a5db-1f0d2caf8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_color = 3\n",
    "max_len = 4\n",
    "empty = 2\n",
    "\n",
    "board = Board([\n",
    "        [0, 1, 2, 0],\n",
    "        [1, 1, 2, 0],\n",
    "        [2, 0, 1, 2],\n",
    "        [],\n",
    "        [],\n",
    "    ], num_color, max_len, empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b67099d4-33c0-4f3e-a18a-504616153425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_color, max_len, empty=2) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear((num_color + empty) * (max_len), 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, (num_color + empty) * (num_color + empty - 1))\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        output = self.fc3(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7db96129-f648-42d4-b4f5-77d0785d1a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  0.],\n",
       "        [ 1.,  1.,  2.,  0.],\n",
       "        [ 2.,  0.,  1.,  2.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.FloatTensor(board)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e3208ee-f85a-4dab-a246-6b24eb3542f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 20]),\n",
       " tensor([[-0.0093, -0.1958, -0.0582, -0.2112,  0.1820, -0.1105,  0.1671,  0.0668,\n",
       "           0.0835,  0.0258, -0.0201,  0.0192, -0.0273, -0.1663, -0.0302, -0.0223,\n",
       "           0.0126,  0.0068, -0.4020, -0.0320]], grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcn = FCN(num_color, max_len, empty)\n",
    "r = fcn(x.unsqueeze(0))\n",
    "r.size(), r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c7076-6039-4f96-b575-337a6b8e977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQAgent:\n",
    "\n",
    "    def __init__(self, epsilon, num_color, max_len, empty, batch_size=64, lr=5e-4, delta=1.0, gamma=0.99):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # The first model makes the predictions for Q-values which make a action.\n",
    "        self.model = FCN(num_color, max_len, empty)\n",
    "        # Build a target model for the prediction of future rewards.\n",
    "        # The weights of a target model is fixed when the first model update weights.  \n",
    "        # Thus when the loss between the Q-values is calculated the target Q-value is stable.\n",
    "        self.target_model = FCN(num_color, max_len, empty)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        # optimizer.\n",
    "        self._lr = lr\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self._lr,\n",
    "            # betas=(meta_conf.beta if hasattr(meta_conf, \"beta\") else 0.9, 0.999),\n",
    "            # weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "        # define criterion function.\n",
    "        # self.criterion = torch.nn.HuberLoss(reduction='mean', delta=1.0)\n",
    "        self.criterion = torch.nn.HuberLoss(delta=delta)\n",
    "        # self.criterion = torch.nn.SmoothL1Loss()\n",
    "\n",
    "        # fixed parameters.\n",
    "        self._gamma = gamma\n",
    "        self._batch_size = batch_size\n",
    "        self._num_actions = (num_color + empty, num_color + empty - 1)\n",
    "\n",
    "    def act(self, board):\n",
    "\n",
    "        \"\"\"This function is used to guide the move of agents.\"\"\"\n",
    "        allowed_actions = board.valid_actions()\n",
    "        if random.random() < self.epsilon:\n",
    "            # take random actions.\n",
    "            action = random.choice(allowed_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                allowed_actions_linear = [i * self._num_actions[0] + j for i, j in allowed_actions]\n",
    "                action_probs = self.model(board.flasks.unsqueeze(0))\n",
    "                action_probs[allowed_actions_linear] = 0\n",
    "                _, action = torch.max(action_probs, 1)\n",
    "                i = action % (self._num_actions[1])\n",
    "                j = action % (self._num_actions[1])\n",
    "                action = (i, j)\n",
    "        return action\n",
    "\n",
    "    def train(self, replay_buffer, update_target=False):\n",
    "        \"\"\"This function is used to update the network parameters.\"\"\"\n",
    "        # retrive batches.\n",
    "        # batch = replay_buffer.get_batch(self._batch_size)\n",
    "        if len(replay_buffer) < replay_buffer.batch_size:\n",
    "            return\n",
    "        transitions = replay_buffer.sample(replay_buffer.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                                batch.next_state)), dtype=torch.bool)\n",
    "        if replay_buffer.batch_size > 1:\n",
    "            non_final_next_states = torch.cat([s.unsqueeze(0) for s in batch.next_state\n",
    "                                        if s is not None], 0)\n",
    "        else:\n",
    "            if batch.next_state[0] is None:\n",
    "                non_final_next_states = None\n",
    "            else:\n",
    "                non_final_next_states = torch.cat([batch.next_state[0].unsqueeze(0)], 0)\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "\n",
    "        next_state_values = torch.zeros(replay_buffer.batch_size)\n",
    "        if non_final_next_states is not None:\n",
    "            next_state_values[non_final_mask] = self.target_model(non_final_next_states).max(1)[0].detach()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        expected_state_action_values = (next_state_values * self._gamma) + reward_batch\n",
    "\n",
    "        # calculate loss.\n",
    "        loss = self.criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if update_target:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class Buffer:\n",
    "    \"\"\"This class is used to instantiate replay buffers.\"\"\"\n",
    "\n",
    "    def __init__(self, capacity, batch_size):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f7798c-24bc-4711-a5f9-fe7c098f1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epsilon_opt, epsilon_learn, DeepQLearner, epoch=500):\n",
    "    env = TictactoeEnv()\n",
    "    Turns = np.array(['O','X'])\n",
    "    reward_sum = 0\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        np.random.seed(i)\n",
    "        \n",
    "        # Our policy makes the first move in the first 250 games\n",
    "        if i < 250:\n",
    "            Turns = np.array(['O','X'])\n",
    "        else:\n",
    "            Turns = np.array(['X','O'])\n",
    "        \n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        player_opt = OptimalPlayer(epsilon=epsilon_opt, player=Turns[0])\n",
    "        player_learn = DeepQLearner\n",
    "        player_learn.epsilon = epsilon_learn\n",
    "        player_learn.player = Turns[1]\n",
    "\n",
    "        for j in range(9):\n",
    "            if env.current_player == player_opt.player:\n",
    "                action = player_opt.act(grid)\n",
    "                grid, end, winner = env.step(action, print_grid=False)\n",
    "            else:\n",
    "                state = grid2state(grid, player_learn.player).to(DEVICE)\n",
    "                action = player_learn.act(state)\n",
    "                # new_position = player_learn.act(grid, q_library)\n",
    "                if check_available(grid2state(grid, player_learn.player), action.item()):\n",
    "                    grid, end, winner = env.step(action.item(), print_grid=False)\n",
    "                else:\n",
    "                    # End the game if the agent takes an unavailable action\n",
    "                    end = True\n",
    "                    # Give the agent a negative reward\n",
    "                    reward = -1\n",
    "                    \n",
    "                    # # Opt wins the games\n",
    "                    env.winner = player_opt.player\n",
    "\n",
    "            if end:\n",
    "                reward_sum += env.reward(player=player_learn.player)\n",
    "                env.reset()\n",
    "                break\n",
    "                \n",
    "    return reward_sum/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d112f17-fcd8-4b18-811f-7f82dda82b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(epsilon_opt, buffer_capacity=10000, batch_size=64, epoch=20000, plot_interval=250, \n",
    "                   colour='black', epsilon_learn_init=None, n_star=None, fig_widge=None, mode=None):\n",
    "\n",
    "    any_epsilon = (epsilon_learn_init is None) and (n_star is None)\n",
    "    assert not any_epsilon, \"Both epsilon_learn and n_star is None! At least having one of them.\"\n",
    "\n",
    "    env = TictactoeEnv()        \n",
    "    Turns = np.array(['X','O'])\n",
    "    plot_interval = 250\n",
    "\n",
    "    # rendering in figure widge\n",
    "    if fig_widge is not None:\n",
    "        init_figure_widge(fig_widge, mode, colour, \n",
    "                          epsilon_opt=epsilon_opt, \n",
    "                          epsilon_learn=epsilon_learn_init, \n",
    "                          n_star=n_star)\n",
    "        \n",
    "    replay_memory = Buffer(capacity=buffer_capacity, batch_size=batch_size)\n",
    "    player_opt = OptimalPlayer(epsilon=epsilon_opt, player=Turns[0])\n",
    "    player_learn = DeepQAgent(epsilon=epsilon_learn_init, player=Turns[1])\n",
    "\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_max = 0.8\n",
    "\n",
    "    reward_sum = 0\n",
    "    loss_sum = 0\n",
    "    loss_step = 0\n",
    "    average_rewards = []\n",
    "    average_losses = []\n",
    "\n",
    "    for i in range(epoch):\n",
    "        # if mode == 'decrease exploration' or 'validation':\n",
    "        if epsilon_learn_init is None:\n",
    "            assert n_star is not None, \"In this setting, we use n_star to tune the epsilon_learn.\"\n",
    "            epsilon_learn = max(epsilon_min, epsilon_max*(1-(i+1)/n_star))\n",
    "        else:\n",
    "            epsilon_learn = epsilon_learn_init\n",
    "        \n",
    "        player_learn.epsilon = epsilon_learn\n",
    "\n",
    "        env.reset()\n",
    "        grid, _, _ = env.observe()\n",
    "\n",
    "        # Switch Order\n",
    "        Turns = Turns[::-1]\n",
    "        player_opt.player = Turns[0]\n",
    "        player_learn.player = Turns[1]\n",
    "        env.current_player  = 'X'\n",
    "\n",
    "        state = None\n",
    "        next_state = None\n",
    "\n",
    "        for j in range(9):\n",
    "            if env.current_player == player_opt.player:\n",
    "                action_opt = player_opt.act(grid)\n",
    "                grid, end, winner = env.step(action_opt, print_grid=False)\n",
    "                \n",
    "                # Get reward\n",
    "                reward = env.reward(player=player_learn.player)\n",
    "                \n",
    "                # In case opt plays first\n",
    "                if j > 0:\n",
    "                    next_state = grid2state(grid, player_learn.player)\n",
    "            else:\n",
    "                state = grid2state(grid, player_learn.player)\n",
    "                action = player_learn.act(state)\n",
    "                action = action.to(DEVICE)\n",
    "\n",
    "                # Check the availability of current action.\n",
    "                if check_available(grid2state(grid, player_learn.player), action.item()):\n",
    "                    grid, end, winner = env.step(action.item(), print_grid=False)\n",
    "\n",
    "                    # Get reward.\n",
    "                    reward = env.reward(player=player_learn.player)\n",
    "                else:\n",
    "                    # End the game if the agent takes an unavailable action\n",
    "                    end = True\n",
    "                    unavailable_action = True\n",
    "                    # Give the agent a negative reward\n",
    "                    reward = -1\n",
    "                    \n",
    "                    # Opponent wins the games\n",
    "                    env.winner = player_opt.player\n",
    "\n",
    "                    \n",
    "            if not end:\n",
    "                # In case opt players first - next_state does not exist\n",
    "                if next_state != None:\n",
    "                    replay_memory.push(state.unsqueeze(0), action, next_state, torch.tensor([reward], device=DEVICE))\n",
    "                    if len(replay_memory) >= replay_memory.batch_size:\n",
    "                        loss = player_learn.train(replay_memory)\n",
    "                        loss_step += 1\n",
    "                        loss_sum += loss\n",
    "                    next_state = None\n",
    "                    \n",
    "            if end:\n",
    "                # Once the game ends, no matter which player plays first\n",
    "                # Update is the same.\n",
    "                if env.winner == player_opt.player:\n",
    "                    # If opt wins the game, reward is guaranteed to be update-to-date\n",
    "                    reward = -1\n",
    "                elif env.winner == player_learn.player:\n",
    "                    # our agent wins.\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    # Draw\n",
    "                    reward = 0\n",
    "\n",
    "                next_state = None\n",
    "                # Update target model every 500 epoch\n",
    "                update_target = False\n",
    "\n",
    "                if (i+1) % 500 == 0:\n",
    "                    update_target = True\n",
    "                replay_memory.push(state.unsqueeze(0), action, next_state, torch.tensor([reward], device=DEVICE))\n",
    "                if len(replay_memory) >= replay_memory.batch_size:\n",
    "                    loss = player_learn.train(replay_memory, update_target)\n",
    "                    loss_step += 1\n",
    "                    loss_sum += loss\n",
    "                    \n",
    "                reward_sum += reward\n",
    "                env.reset()\n",
    "                break       \n",
    "            \n",
    "        #############################\n",
    "        ######### Plot ##############\n",
    "\n",
    "        if (i+1) % plot_interval == 0:\n",
    "            # calculate average reward at the end of the current interval.\n",
    "            average_reward = reward_sum / plot_interval  \n",
    "            average_loss = loss_sum / loss_step\n",
    "            # print(average_reward)  \n",
    "            # print(average_loss)\n",
    "            \n",
    "            if not 'validate' in mode:\n",
    "                idx = len(fig_widge.data)-2\n",
    "                \n",
    "                fig_widge.data[idx].x = np.append(fig_widge.data[idx].x, i+1)[0:]\n",
    "                fig_widge.data[idx].y = np.append(fig_widge.data[idx].y, average_reward)[0:]\n",
    "                \n",
    "                fig_widge.data[idx+1].x = np.append(fig_widge.data[idx+1].x, i+1)[0:]\n",
    "                fig_widge.data[idx+1].y = np.append(fig_widge.data[idx+1].y, average_loss.item())[0:]\n",
    "                \n",
    "            else:\n",
    "                M_opt = validation(0, 0, player_learn, epoch=500)\n",
    "                M_rand = validation(1, 0, player_learn, epoch=500)\n",
    "                \n",
    "                idx = len(fig_widge.data)-2\n",
    "                fig_widge.data[idx].x = np.append(fig_widge.data[idx].x, i+1)[0:]\n",
    "                fig_widge.data[idx].y = np.append(fig_widge.data[idx].y, M_opt)[0:]\n",
    "                fig_widge.data[idx+1].x = np.append(fig_widge.data[idx+1].x, i+1)[0:]\n",
    "                fig_widge.data[idx+1].y = np.append(fig_widge.data[idx+1].y, M_rand)[0:]\n",
    "                fig_widge.layout.title.text = f'Epoch {i+1}, M_opt = {M_opt}, M_rand = {M_rand}'\n",
    "\n",
    "            average_rewards.append(average_reward)\n",
    "            average_losses.append(average_loss)\n",
    "            # reset reward_sum.\n",
    "            reward_sum = 0\n",
    "            loss_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea78fa1-4e99-4605-a1a3-c874d2206bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_practice_simulation(epsilon_opt=None, epoch=20000, plot_interval=250, \n",
    "                   colour='black', epsilon_learn_init=None, n_star=None, fig_widge=None, mode=None):\n",
    "\n",
    "    any_epsilon = (epsilon_learn_init is None) and (n_star is None)\n",
    "    assert not any_epsilon, \"Both epsilon_learn and n_star is None! At least having one of them.\"\n",
    "\n",
    "    env = TictactoeEnv()        \n",
    "    Turns = np.array(['X','O'])\n",
    "    plot_interval = 250\n",
    "\n",
    "    # rendering in figure widge\n",
    "    if fig_widge is not None:\n",
    "        if n_star is None:\n",
    "            fig_widge.add_scatter(x=[0], y=[0], name=f'M-opt vs. Q-agent, epsilon = {epsilon_learn_init}', marker=dict(color=colour))\n",
    "            fig_widge.add_scatter(x=[0], y=[0], name=f'M-rand vs. Q-agent, epsilon = {epsilon_learn_init}', marker=dict(color=colour), line=dict(dash='dot'))\n",
    "        else:\n",
    "            fig_widge.add_scatter(x=[0], y=[0], name=f'M-opt vs. Q-agent, n* = {n_star}', marker=dict(color=colour))\n",
    "            fig_widge.add_scatter(x=[0], y=[0], name=f'M-rand vs. Q-agent, n* = {n_star}', marker=dict(color=colour), line=dict(dash='dot'))\n",
    "        \n",
    "    replay_memory = Buffer(capacity=10000, batch_size=64)\n",
    "    shared_model = FCN()\n",
    "    # Use player_A as our agent. Need to exchange order when using player_B.\n",
    "    player_A = DeepQAgent(epsilon=epsilon_learn_init, player='O', model=shared_model)\n",
    "    player_B = DeepQAgent(epsilon=epsilon_learn_init, player='X', model=shared_model)\n",
    "\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_max = 0.8\n",
    "\n",
    "    reward_sum_A = 0\n",
    "    loss_sum_A = 0\n",
    "    loss_step_A = 0\n",
    "    average_rewards_A = []\n",
    "    average_losses_A = []\n",
    "\n",
    "    for i in range(epoch):\n",
    "        # if mode == 'decrease exploration' or 'validation':\n",
    "        if epsilon_learn_init is None:\n",
    "            assert n_star is not None, \"In this setting, we use n_star to tune the epsilon_learn.\"\n",
    "            epsilon_learn = max(epsilon_min, epsilon_max*(1-(i+1)/n_star))\n",
    "        else:\n",
    "            epsilon_learn = epsilon_learn_init\n",
    "        \n",
    "        player_A.epsilon = epsilon_learn\n",
    "        player_B.epsilon = epsilon_learn\n",
    "\n",
    "        env.reset()\n",
    "        grid, _, _ = env.observe()\n",
    "\n",
    "        # Switch Order\n",
    "        # Turns = Turns[::-1] \n",
    "        # env.current_player = Turns[0]\n",
    "        Turns = Turns[::-1]\n",
    "        player_A.player = Turns[0]\n",
    "        player_B.player = Turns[1]\n",
    "        env.current_player  = 'X'\n",
    "\n",
    "        state_A = None\n",
    "        next_state_A = None\n",
    "        state_B = None\n",
    "        next_state_B = None\n",
    "\n",
    "        for j in range(9):\n",
    "            if env.current_player == player_A.player:\n",
    "                # state_A is observed from the perspective of A.\n",
    "                state_A = grid2state(grid, player_A.player)\n",
    "                action_A = player_A.act(state_A)\n",
    "                action_A = action_A.to(DEVICE)\n",
    "                \n",
    "                # Get reward\n",
    "                if check_available(grid2state(grid, player_A.player), action_A.item()):\n",
    "                    grid, end, winner = env.step(action_A.item(), print_grid=False)\n",
    "\n",
    "                    # Get reward.\n",
    "                    reward_A = env.reward(player=player_A.player)\n",
    "                    # In case A plays first.\n",
    "                    if j > 0:\n",
    "                        next_state_B = grid2state(grid, player_B.player)\n",
    "                else:\n",
    "                    # End the game if the agent takes an unavailable action\n",
    "                    end = True\n",
    "                    # Give the agent a negative reward\n",
    "                    reward_A = -1\n",
    "                    \n",
    "                    # # Opt wins the games\n",
    "                    env.winner = player_B.player\n",
    "\n",
    "            else:\n",
    "                # state_B is observed from the perspective of B.\n",
    "                state_B = grid2state(grid, player_B.player)\n",
    "                action_B = player_B.act(state_B)\n",
    "                action_B = action_B.to(DEVICE)\n",
    "\n",
    "                if check_available(grid2state(grid, player_B.player), action_B.item()):\n",
    "                    grid, end, winner = env.step(action_B.item(), print_grid=False)\n",
    "\n",
    "                    # Get reward.\n",
    "                    reward_B = env.reward(player=player_B.player)\n",
    "                    # In case A plays first\n",
    "                    if j > 0:\n",
    "                        next_state_A = grid2state(grid, player_A.player)\n",
    "                else:\n",
    "                    # End the game if the agent takes an unavailable action\n",
    "                    end = True\n",
    "                    # Give the agent a negative reward\n",
    "                    reward_B = -1\n",
    "                    \n",
    "                    # # Opt wins the games\n",
    "                    env.winner = player_A.player\n",
    "    \n",
    "            if not end:\n",
    "                # In case opt players first - next_state does not exist\n",
    "                if next_state_A != None:\n",
    "                    replay_memory.push(state_A.unsqueeze(0), action_A, next_state_A, torch.tensor([reward_A], device=DEVICE))\n",
    "                    if len(replay_memory) >= replay_memory.batch_size:\n",
    "                        loss_A = player_A.train(replay_memory)\n",
    "                        loss_sum_A += loss_A\n",
    "                        loss_step_A += 1\n",
    "                    next_state_A = None\n",
    "                \n",
    "                if next_state_B != None:\n",
    "                    replay_memory.push(state_B.unsqueeze(0), action_B, next_state_B, torch.tensor([reward_B], device=DEVICE))\n",
    "                    if len(replay_memory) >= replay_memory.batch_size:\n",
    "                        _ = player_B.train(replay_memory)\n",
    "                    next_state_B = None\n",
    "                          \n",
    "            if end:\n",
    "                # Once ending the game, no matter which player plays first\n",
    "                # Update is the same\n",
    "                if env.winner == player_A.player:\n",
    "                    # If opt wins the game, reward is guaranteed to be update-to-date\n",
    "                    reward_A = 1\n",
    "                    reward_B = -1\n",
    "                elif env.winner == player_B.player:\n",
    "                    reward_A = -1\n",
    "                    reward_B = 1\n",
    "                else:\n",
    "                    # Draw\n",
    "                    reward_A = 0\n",
    "                    reward_B = 0\n",
    "                \n",
    "                # If one agent wins the game, we don't actually real next_state\n",
    "                # Because q(s', a') will be cancelled out.\n",
    "                next_state_A = None\n",
    "                next_state_B = None\n",
    "                \n",
    "                replay_memory.push(state_A.unsqueeze(0), action_A, next_state_A, torch.tensor([reward_A], device=DEVICE))\n",
    "                replay_memory.push(state_B.unsqueeze(0), action_B, next_state_B, torch.tensor([reward_B], device=DEVICE))\n",
    "                \n",
    "                # Update target model every 500 epoch.\n",
    "                update_target = False\n",
    "\n",
    "                if (i+1) % 500 == 0:\n",
    "                    update_target = True\n",
    "\n",
    "                # Training model when game over, and replay_memory size >= batch size\n",
    "                if len(replay_memory) >= replay_memory.batch_size:\n",
    "                    loss_A = player_A.train(replay_memory, update_target)\n",
    "                    loss_sum_A += loss_A\n",
    "                    loss_step_A += 1\n",
    "                    _ = player_B.train(replay_memory, update_target)\n",
    "\n",
    "                    \n",
    "                # always focus on the average reward of player_A.\n",
    "                reward_sum_A += reward_A\n",
    "                env.reset()\n",
    "                break       \n",
    "            \n",
    "        #############################\n",
    "        ######### Plot ##############\n",
    "\n",
    "        if (i+1) % plot_interval == 0:\n",
    "            # calculate average reward at the end of the current interval.\n",
    "            average_reward_A = reward_sum_A / plot_interval  \n",
    "            average_loss_A = loss_sum_A / loss_step_A\n",
    "            print(average_reward_A)  \n",
    "            print(average_loss_A)\n",
    "            \n",
    "            M_opt_1 = validation(0, 0, player_A, epoch=500)\n",
    "            M_rand_1 = validation(1, 0, player_A, epoch=500)\n",
    "\n",
    "            idx = len(fig_widge.data)-2\n",
    "            fig_widge.data[idx].x = np.append(fig_widge.data[idx].x, i+1)[0:]\n",
    "            fig_widge.data[idx].y = np.append(fig_widge.data[idx].y, M_opt_1)[0:]\n",
    "            fig_widge.data[idx+1].x = np.append(fig_widge.data[idx+1].x, i+1)[0:]\n",
    "            fig_widge.data[idx+1].y = np.append(fig_widge.data[idx+1].y, M_rand_1)[0:]\n",
    "            fig_widge.layout.title.text = f'Epoch {i+1}, M_opt = {M_opt_1}, M_rand = {M_rand_1}'\n",
    "\n",
    "            average_rewards_A.append(average_reward_A)\n",
    "            average_losses_A.append(average_loss_A)\n",
    "            # reset reward_sum.\n",
    "            reward_sum_A = 0\n",
    "            loss_sum_A = 0\n",
    "            loss_step_A = 0\n",
    "\n",
    "    return (shared_model, replay_memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
