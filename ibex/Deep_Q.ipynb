{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mrrejmZfnufq",
   "metadata": {
    "id": "mrrejmZfnufq"
   },
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <h4 class=\"alert-heading\">Reminder!</h4>\n",
    "    <p>Our team uses Plotly to visualise reward in a continuous manner, therefore:</p>\n",
    "    <ul>\n",
    "      <li>If you have Plotly and ipywidgets installed in your environment, skip the following code cell</li>\n",
    "      <li>Otherwise, run the following code cell and <strong>restart your jupyter notebook!</strong></li>\n",
    "    <ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3a9b26",
   "metadata": {
    "id": "8f3a9b26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-5.11.0-py2.py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.1.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.11.0 tenacity-8.1.0\n",
      "Requirement already satisfied: ipywidgets in /home/alghra0e/rl/env/lib/python3.9/site-packages (8.0.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipywidgets) (5.5.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipywidgets) (4.0.3)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipywidgets) (6.16.2)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipywidgets) (3.0.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipywidgets) (8.5.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.4)\n",
      "Requirement already satisfied: packaging in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (21.3)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.3)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: psutil in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.3)\n",
      "Requirement already satisfied: stack-data in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.31)\n",
      "Requirement already satisfied: pickleshare in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: decorator in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.17.2)\n",
      "Requirement already satisfied: backcall in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.13.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.11.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /home/alghra0e/rl/env/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/alghra0e/rl/env/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from packaging->ipykernel>=4.5.1->ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: executing in /home/alghra0e/rl/env/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: asttokens in /home/alghra0e/rl/env/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.8)\n",
      "Requirement already satisfied: pure-eval in /home/alghra0e/rl/env/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/alghra0e/rl/env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "423fa7e1",
   "metadata": {
    "id": "423fa7e1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51aae394-aa94-4670-93f4-340c3996b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "class TictactoeEnv:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Classical Tic-tac-toe game for two players who take turns marking the spaces in a three-by-three grid with X or O.\n",
    "        The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row is the winner.\n",
    "\n",
    "        The game is played by two players: player 'X' and player 'O'. Player 'x' moves first.\n",
    "\n",
    "        The grid is represented by a 3x3 numpy array, with value in {0, 1, -1}, with corresponding values:\n",
    "            0 - place unmarked\n",
    "            1 - place marked with X\n",
    "            -1 - place marked with O\n",
    "\n",
    "        The game environment will recieve movement from two players in turn and update the grid.\n",
    "\n",
    "    self.step:\n",
    "        recieve the movement of the player, update the grid\n",
    "\n",
    "    The action space is [0-8], representing the 9 positions on the grid.\n",
    "\n",
    "    The reward is 1 if you win the game, -1 if you lose, and 0 besides.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.grid = np.zeros((3, 3))\n",
    "        self.end = False\n",
    "        self.winner = None\n",
    "        self.player2value = {'X': 1, 'O': -1}\n",
    "        self.num_step = 0\n",
    "        self.current_player = 'X'  # By default, player 'X' goes first\n",
    "\n",
    "    def check_valid(self, position):\n",
    "        \"\"\"Check whether the current action is valid or not\"\"\"\n",
    "        if self.end:\n",
    "            raise ValueError('This game has ended, please reset it!')\n",
    "        if type(position) is int:\n",
    "            position = (int(position / 3), position % 3)\n",
    "        elif type(position) is not tuple:\n",
    "            position = tuple(position)\n",
    "\n",
    "        return False if self.grid[position] != 0 else True\n",
    "\n",
    "    def step(self, position, print_grid=False):\n",
    "        \"\"\" Receive the movement from two players in turn and update the grid\"\"\"\n",
    "        # check the position and value are valid or not\n",
    "        # position should be a tuple like (0, 1) or int [0-8]\n",
    "\n",
    "        if self.end:\n",
    "            raise ValueError('This game has ended, please reset it!')\n",
    "        if type(position) is int:\n",
    "            position = (int(position / 3), position % 3)\n",
    "        elif type(position) is not tuple:\n",
    "            position = tuple(position)\n",
    "        if self.grid[position] != 0:\n",
    "            raise ValueError('There is already a chess on position {}.'.format(position))\n",
    "\n",
    "        # place a chess on the position\n",
    "        self.grid[position] = self.player2value[self.current_player]\n",
    "\n",
    "        # update\n",
    "        self.num_step += 1\n",
    "\n",
    "        ############### MODIFIED SWITCH ORDER ###########################\n",
    "        self.current_player = 'X' if self.current_player == 'O' else 'O'\n",
    "        # self.current_player = 'X' if self.num_step % 2 == 0 else  'O'\n",
    "\n",
    "        # check whether the game ends or not\n",
    "        self.checkEnd()\n",
    "\n",
    "        if print_grid:\n",
    "            self.render()\n",
    "\n",
    "        return self.grid.copy(), self.end, self.winner\n",
    "\n",
    "    def get_current_player(self):\n",
    "        return self.current_player\n",
    "\n",
    "    def checkEnd(self):\n",
    "        # check rows and cols\n",
    "        if np.any(np.sum(self.grid, axis=0) == 3) or np.any(np.sum(self.grid, axis=1) == 3):\n",
    "            self.end = True\n",
    "            self.winner = 'X'\n",
    "        elif np.any(np.sum(self.grid, axis=0) == -3) or np.any(np.sum(self.grid, axis=1) == -3):\n",
    "            self.end = True\n",
    "            self.winner = 'O'\n",
    "        # check diagnols\n",
    "        elif self.grid[[0, 1, 2], [0, 1, 2]].sum() == 3 or self.grid[[0, 1, 2], [2, 1, 0]].sum() == 3:\n",
    "            self.end = True\n",
    "            self.winner = 'X'\n",
    "        elif self.grid[[0, 1, 2], [0, 1, 2]].sum() == -3 or self.grid[[0, 1, 2], [2, 1, 0]].sum() == -3:\n",
    "            self.end = True\n",
    "            self.winner = 'O'\n",
    "        # check if all the positions are filled\n",
    "        elif (self.grid == 0).sum() == 0:\n",
    "            self.end = True\n",
    "            self.winner = None  # no one wins\n",
    "        else:\n",
    "            self.end = False\n",
    "            self.winner = None\n",
    "\n",
    "    def reset(self):\n",
    "        # reset the grid\n",
    "        self.grid = np.zeros((3, 3))\n",
    "        self.end = False\n",
    "        self.winner = None\n",
    "        self.num_step = 0\n",
    "        self.current_player = 'X'\n",
    "\n",
    "        return self.grid.copy(), self.end, self.winner\n",
    "\n",
    "    def observe(self):\n",
    "        return self.grid.copy(), self.end, self.winner\n",
    "\n",
    "    def reward(self, player='X'):\n",
    "        if self.end:\n",
    "            if self.winner is None:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1 if player == self.winner else -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def render(self):\n",
    "        # print current grid\n",
    "        value2player = {0: '-', 1: 'X', -1: 'O'}\n",
    "        for i in range(3):\n",
    "            print('|', end='')\n",
    "            for j in range(3):\n",
    "                print(value2player[int(self.grid[i, j])], end=' ' if j < 2 else '')\n",
    "            print('|')\n",
    "        print()\n",
    "\n",
    "\n",
    "class OptimalPlayer:\n",
    "    '''\n",
    "    Description:\n",
    "        A class to implement an epsilon-greedy optimal player in Tic-tac-toe.\n",
    "\n",
    "    About optimial policy:\n",
    "        There exists an optimial policy for game Tic-tac-toe. A player ('X' or 'O') can win or at least draw with optimial strategy.\n",
    "        See the wikipedia page for details https://en.wikipedia.org/wiki/Tic-tac-toe\n",
    "        In short, an optimal player choose the first available move from the following list:\n",
    "            [Win, BlockWin, Fork, BlockFork, Center, Corner, Side]\n",
    "\n",
    "    Parameters:\n",
    "        epsilon: float, in [0, 1]. This is a value between 0-1 that indicates the\n",
    "            probability of making a random action instead of the optimal action\n",
    "            at any given time.\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, epsilon=0.2, player='X'):\n",
    "        self.epsilon = epsilon\n",
    "        self.player = player  # 'x' or 'O'\n",
    "\n",
    "    def set_player(self, player='X', j=-1):\n",
    "        self.player = player\n",
    "        if j != -1:\n",
    "            self.player = 'X' if j % 2 == 0 else 'O'\n",
    "\n",
    "    def empty(self, grid):\n",
    "        '''return all empty positions'''\n",
    "        avail = []\n",
    "        for i in range(9):\n",
    "            pos = (int(i / 3), i % 3)\n",
    "            if grid[pos] == 0:\n",
    "                avail.append(pos)\n",
    "        return avail\n",
    "\n",
    "    def center(self, grid):\n",
    "        '''\n",
    "        Pick the center if its available,\n",
    "        if it's the first step of the game, center or corner are all optimial.\n",
    "        '''\n",
    "        if np.abs(grid).sum() == 0:\n",
    "            # first step of the game\n",
    "            return [(1, 1)] + self.corner(grid)\n",
    "\n",
    "        return [(1, 1)] if grid[1, 1] == 0 else []\n",
    "\n",
    "    def corner(self, grid):\n",
    "        ''' Pick empty corners to move '''\n",
    "        corner = [(0, 0), (0, 2), (2, 0), (2, 2)]\n",
    "        cn = []\n",
    "        # First, pick opposite corner of opponent if it's available\n",
    "        for i in range(4):\n",
    "            if grid[corner[i]] == 0 and grid[corner[3 - i]] != 0:\n",
    "                cn.append(corner[i])\n",
    "        if cn != []:\n",
    "            return cn\n",
    "        else:\n",
    "            for idx in corner:\n",
    "                if grid[idx] == 0:\n",
    "                    cn.append(idx)\n",
    "            return cn\n",
    "\n",
    "    def side(self, grid):\n",
    "        ''' Pick empty sides to move'''\n",
    "        rt = []\n",
    "        for idx in [(0, 1), (1, 0), (1, 2), (2, 1)]:\n",
    "            if grid[idx] == 0:\n",
    "                rt.append(idx)\n",
    "        return rt\n",
    "\n",
    "    def win(self, grid, val=None):\n",
    "        ''' Pick all positions that player will win after taking it'''\n",
    "        if val is None:\n",
    "            val = 1 if self.player == 'X' else -1\n",
    "\n",
    "        towin = []\n",
    "        # check all positions\n",
    "        for pos in self.empty(grid):\n",
    "            grid_ = np.copy(grid)\n",
    "            grid_[pos] = val\n",
    "            if self.checkWin(grid_, val):\n",
    "                towin.append(pos)\n",
    "\n",
    "        return towin\n",
    "\n",
    "    def blockWin(self, grid):\n",
    "        ''' Find the win positions of opponent and block it'''\n",
    "        oppon_val = -1 if self.player == 'X' else 1\n",
    "        return self.win(grid, oppon_val)\n",
    "\n",
    "    def fork(self, grid, val=None):\n",
    "        ''' Find a fork opportunity that the player will have two positions to win'''\n",
    "        if val is None:\n",
    "            val = 1 if self.player == 'X' else -1\n",
    "\n",
    "        tofork = []\n",
    "        # check all positions\n",
    "        for pos in self.empty(grid):\n",
    "            grid_ = np.copy(grid)\n",
    "            grid_[pos] = val\n",
    "            if self.checkFork(grid_, val):\n",
    "                tofork.append(pos)\n",
    "\n",
    "        return tofork\n",
    "\n",
    "    def blockFork(self, grid):\n",
    "        ''' Block the opponent's fork.\n",
    "            If there is only one possible fork from opponent, block it.\n",
    "            Otherwise, player should force opponent to block win by making two in a row or column\n",
    "            Amomg all possible force win positions, choose positions in opponent's fork in prior\n",
    "        '''\n",
    "        oppon_val = -1 if self.player == 'X' else 1\n",
    "        oppon_fork = self.fork(grid, oppon_val)\n",
    "        if len(oppon_fork) <= 1:\n",
    "            return oppon_fork\n",
    "\n",
    "        # force the opponent to block win\n",
    "        force_blockwin = []\n",
    "        val = 1 if self.player == 'X' else -1\n",
    "        for pos in self.empty(grid):\n",
    "            grid_ = np.copy(grid)\n",
    "            grid_[pos] = val\n",
    "            if np.any(np.sum(grid_, axis=0) == val * 2) or np.any(np.sum(grid_, axis=1) == val * 2):\n",
    "                force_blockwin.append(pos)\n",
    "        force_blockwin_prior = []\n",
    "        for pos in force_blockwin:\n",
    "            if pos in oppon_fork:\n",
    "                force_blockwin_prior.append(pos)\n",
    "\n",
    "        return force_blockwin_prior if force_blockwin_prior != [] else force_blockwin\n",
    "\n",
    "    def checkWin(self, grid, val=None):\n",
    "        # check whether the player corresponding to the val will win\n",
    "        if val is None:\n",
    "            val = 1 if self.player == 'X' else -1\n",
    "        target = 3 * val\n",
    "        # check rows and cols\n",
    "        if np.any(np.sum(grid, axis=0) == target) or np.any(np.sum(grid, axis=1) == target):\n",
    "            return True\n",
    "        # check diagnols\n",
    "        elif grid[[0, 1, 2], [0, 1, 2]].sum() == target or grid[[0, 1, 2], [2, 1, 0]].sum() == target:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def checkFork(self, grid, val=None):\n",
    "        # check whether the player corresponding to the val will fork\n",
    "        if val is None:\n",
    "            val = 1 if self.player == 'X' else -1\n",
    "        target = 2 * val\n",
    "        # check rows and cols\n",
    "        rows = (np.sum(grid, axis=0) == target).sum()\n",
    "        cols = (np.sum(grid, axis=1) == target).sum()\n",
    "        diags = (grid[[0, 1, 2], [0, 1, 2]].sum() == target) + (grid[[0, 1, 2], [2, 1, 0]].sum() == target)\n",
    "        if (rows + cols + diags) >= 2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def randomMove(self, grid):\n",
    "        \"\"\" Chose a random move from the available options. \"\"\"\n",
    "        avail = self.empty(grid)\n",
    "\n",
    "        return avail[random.randint(0, len(avail) - 1)]\n",
    "\n",
    "    def act(self, grid, **kwargs):\n",
    "        \"\"\"\n",
    "        Goes through a hierarchy of moves, making the best move that\n",
    "        is currently available each time (with probabitity 1-self.epsilon).\n",
    "        A touple is returned that represents (row, col).\n",
    "        \"\"\"\n",
    "        # whether move in random or not\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.randomMove(grid)\n",
    "\n",
    "        ### optimial policies\n",
    "\n",
    "        # Win\n",
    "        win = self.win(grid)\n",
    "        if win != []:\n",
    "            return win[random.randint(0, len(win) - 1)]\n",
    "        # Block win\n",
    "        block_win = self.blockWin(grid)\n",
    "        if block_win != []:\n",
    "            return block_win[random.randint(0, len(block_win) - 1)]\n",
    "        # Fork\n",
    "        fork = self.fork(grid)\n",
    "        if fork != []:\n",
    "            return fork[random.randint(0, len(fork) - 1)]\n",
    "        # Block fork\n",
    "        block_fork = self.blockFork(grid)\n",
    "        if block_fork != []:\n",
    "            return block_fork[random.randint(0, len(block_fork) - 1)]\n",
    "        # Center\n",
    "        center = self.center(grid)\n",
    "        if center != []:\n",
    "            return center[random.randint(0, len(center) - 1)]\n",
    "        # Corner\n",
    "        corner = self.corner(grid)\n",
    "        if corner != []:\n",
    "            return corner[random.randint(0, len(corner) - 1)]\n",
    "        # Side\n",
    "        side = self.side(grid)\n",
    "        if side != []:\n",
    "            return side[random.randint(0, len(side) - 1)]\n",
    "\n",
    "        # random move\n",
    "        return self.randomMove(grid)\n",
    "\n",
    "\n",
    "def grid_to_state(grid):\n",
    "    \"\"\"Convert ndarray to flattened python list\"\"\"\n",
    "    return grid.flatten().tolist()\n",
    "\n",
    "\n",
    "def state_to_grid(state):\n",
    "    \"\"\"Restore flattened python list back to 3*3 grid\"\"\"\n",
    "    return np.asarray(state).reshape((3, 3))\n",
    "\n",
    "\n",
    "class QAgent(OptimalPlayer):\n",
    "    def __init__(self, epsilon=0.2, player='X'):\n",
    "        super(QAgent, self).__init__(epsilon, player)\n",
    "        self.player = player\n",
    "        self.curr_state = None\n",
    "\n",
    "    def act(self, grid, learner_lib):\n",
    "\n",
    "        # In act, states and q_table are used but not updated.\n",
    "        state_table = learner_lib.states\n",
    "        q_table = learner_lib.q_table\n",
    "\n",
    "        # Convert 3*3 ndarray to list for reference\n",
    "        self.curr_state = grid_to_state(grid)\n",
    "\n",
    "        # Retrieve q-values of the current state\n",
    "        # state_idx = state_table.index(self.curr_state)\n",
    "\n",
    "        # print(f'state_idx {state_idx}')\n",
    "        # action_qvalues = q_table[state_idx]\n",
    "        # print(f'action values {action_qvalues}')\n",
    "\n",
    "        # Exploitation\n",
    "        avail_actions = self.avail_action_from_state(self.curr_state, state_table, q_table)\n",
    "        best_action = avail_actions[0]\n",
    "\n",
    "        # Exploration\n",
    "        if random.random() < self.epsilon:\n",
    "            best_action = random.choice(avail_actions)\n",
    "            # print(f\"Exploration random {best_action}\")\n",
    "\n",
    "        return int(best_action)\n",
    "\n",
    "    def avail_action_from_state(self, state, state_table, q_table):\n",
    "        \"\"\"\n",
    "        Return available action indexes according to their qvalue in descending order.\n",
    "        \"\"\"\n",
    "        avail = []\n",
    "\n",
    "        state_idx = state_table.index(state)\n",
    "        action_qvalues = q_table[state_idx]\n",
    "\n",
    "        # Retrieve a list of action index with action q-value in descending order\n",
    "        # Here action idx is actually position on game board\n",
    "        sorted_action_idx = np.flip(np.argsort(action_qvalues))\n",
    "\n",
    "        # Retrieve the available action indexes\n",
    "        # Action is not available if position is already taken\n",
    "        for idx in sorted_action_idx:\n",
    "            if state[idx] == 0:\n",
    "                avail.append(idx)\n",
    "\n",
    "        return avail\n",
    "\n",
    "    def update_states(self, state, learner_lib):\n",
    "\n",
    "        if state not in learner_lib.states:\n",
    "            learner_lib.add_state(state)\n",
    "            learner_lib.add_q()\n",
    "\n",
    "    def update_q(self, state, action, next_state, reward, learner_lib):\n",
    "\n",
    "        curr_state_idx = learner_lib.states.index(state)\n",
    "\n",
    "        if next_state is not None:\n",
    "            next_state_idx = learner_lib.states.index(next_state)\n",
    "\n",
    "            next_action = \\\n",
    "                self.avail_action_from_state(next_state, learner_lib.states, learner_lib.q_table)[0]\n",
    "            greedy_qsa = learner_lib.q_table[next_state_idx][next_action]\n",
    "\n",
    "            learner_lib.update_q(curr_state_idx, action, reward, greedy_qsa=greedy_qsa)\n",
    "        else:\n",
    "            learner_lib.update_q(curr_state_idx, action, reward, greedy_qsa=0)\n",
    "\n",
    "\n",
    "class QLibrary:\n",
    "    \"\"\"\n",
    "    This class is used to store the changing states and Q values. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.05, gamma=0.99) -> None:\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "\n",
    "        self.states = [[0] * 9]\n",
    "        self.q_table = [[0] * 9]  # 9 actions, each position is an action\n",
    "\n",
    "    def add_state(self, new_state):\n",
    "        \"\"\"\n",
    "        Setter Equivalent\n",
    "        \"\"\"\n",
    "        self.states.append(new_state)\n",
    "\n",
    "    def add_q(self):\n",
    "        \"\"\"\n",
    "        Setter Equivalent\n",
    "        \"\"\"\n",
    "        self.q_table.append([0] * 9)\n",
    "\n",
    "    def update_q(self, state_idx, action_idx, reward, greedy_qsa):\n",
    "        \"\"\"\n",
    "        This function is used to update Q values according to reward and Q values of next step.\n",
    "\n",
    "        params:\n",
    "            state_idx: index of state. Used to retrive the specified Q values.\n",
    "            action_idx: index of action. Used to retrive the specified Q values.\n",
    "            greedy_qsa: maximal Q values of next step. 0 at terminal states.\n",
    "            reward: reward after current action step.\n",
    "        \"\"\"\n",
    "        qsa = self.q_table[state_idx][action_idx]\n",
    "        # print(qsa)\n",
    "\n",
    "        self.q_table[state_idx][action_idx] = qsa + self._alpha * (reward + self._gamma * greedy_qsa - qsa)\n",
    "        # print(self.q_table[state_idx][action_idx])        \n",
    "\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    \"\"\"\n",
    "    This class is to build the neural network for inferencing the next step.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(18, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 9)\n",
    "        self.relu = torch.nn.ReLU()  # instead of Heaviside step fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        output = self.fc3(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class DeepQAgent:\n",
    "    \"\"\"\n",
    "    This class is used to instantiate an agent for DQN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon, player='X') -> None:\n",
    "        # setup.\n",
    "        self.epsilon = epsilon\n",
    "        self.player = player\n",
    "\n",
    "        # The first model makes the predictions for Q-values which make a action.\n",
    "        self.model = FCN()\n",
    "        # Build a target model for the prediction of future rewards.\n",
    "        # The weights of a target model is fixed when the first model update weights.  \n",
    "        # Thus when the loss between the Q-values is calculated the target Q-value is stable.\n",
    "        self.target_model = FCN()\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        # optimizer.\n",
    "        self._lr = 5e-4\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self._lr,\n",
    "            # betas=(meta_conf.beta if hasattr(meta_conf, \"beta\") else 0.9, 0.999),\n",
    "            # weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "        # define criterion function.\n",
    "        # self.criterion = torch.nn.HuberLoss(reduction='mean', delta=1.0)\n",
    "        self.criterion = torch.nn.HuberLoss(delta=1.0)\n",
    "        # self.criterion = torch.nn.SmoothL1Loss()\n",
    "\n",
    "        # fixed parameters.\n",
    "        self._gamma = 0.99\n",
    "        self._batch_size = 64\n",
    "        self._num_actions = 9\n",
    "\n",
    "    def act(self, grid):\n",
    "\n",
    "        \"\"\"This function is used to guide the move of agents.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            # take random actions.\n",
    "            action = torch.randint(self._num_actions, (1,))\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action_probs = self.model(grid.unsqueeze(0))\n",
    "                _, action = torch.max(action_probs, 1)\n",
    "        return action\n",
    "\n",
    "    def train(self, replay_buffer, update_target=False):\n",
    "        \"\"\"This function is used to update the network parameters.\"\"\"\n",
    "        # retrive batches.\n",
    "        # batch = replay_buffer.get_batch(self._batch_size)\n",
    "        if len(replay_buffer) < replay_buffer.batch_size:\n",
    "            return\n",
    "        transitions = replay_buffer.sample(replay_buffer.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                                batch.next_state)), dtype=torch.bool)\n",
    "        if replay_buffer.batch_size > 1:\n",
    "            non_final_next_states = torch.cat([s.unsqueeze(0) for s in batch.next_state\n",
    "                                        if s is not None], 0)\n",
    "        else:\n",
    "            if batch.next_state[0] is None:\n",
    "                non_final_next_states = None\n",
    "            else:\n",
    "                non_final_next_states = torch.cat([batch.next_state[0].unsqueeze(0)], 0)\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        # states = torch.cat([batch[i]['state'].unsqueeze(0) for i in range(len(batch))], 0)\n",
    "        # next_states = torch.cat([batch[i]['next_state'].unsqueeze(0) for i in range(len(batch))], 0)\n",
    "        # rewards = torch.FloatTensor([batch[i]['reward'] for i in range(len(batch))])\n",
    "        # actions = torch.LongTensor([batch[i]['action'] for i in range(len(batch))])\n",
    "        # dones = torch.FloatTensor([batch[i]['done'] for i in range(len(batch))])\n",
    "        # masks = F.one_hot(actions, self._num_actions)\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "        # calculate target values.\n",
    "        # with torch.no_grad():\n",
    "        #     future_rewards = self.target_model(next_states)\n",
    "        next_state_values = torch.zeros(replay_buffer.batch_size)\n",
    "        if non_final_next_states is not None:\n",
    "            next_state_values[non_final_mask] = self.target_model(non_final_next_states).max(1)[0].detach()\n",
    "        else:\n",
    "            pass\n",
    "        # target_q_values = (1. - dones) * (rewards + self._gamma * torch.max(future_rewards, dim=1)[0]) - dones\n",
    "        # target_q_values = rewards + (1 - dones) * self._gamma * torch.max(future_rewards, dim=1)[0]\n",
    "        expected_state_action_values = (next_state_values * self._gamma) + reward_batch\n",
    "\n",
    "        # update action model prameters.\n",
    "        # q_values = self.model(states)\n",
    "        # qsa = torch.sum(torch.mul(q_values, masks), dim=1)\n",
    "        # qsa = self.model(states).gather(1, actions)\n",
    "\n",
    "        # calculate loss.\n",
    "        loss = self.criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if update_target:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class Buffer:\n",
    "    \"\"\"This class is used to instantiate replay buffers.\"\"\"\n",
    "\n",
    "    def __init__(self, capacity, batch_size):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O8JZ3N6xqIE_",
   "metadata": {
    "id": "O8JZ3N6xqIE_"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c407759",
   "metadata": {
    "id": "4c407759"
   },
   "outputs": [],
   "source": [
    "colours = px.colors.qualitative.Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "t0gCXm6ncZIn",
   "metadata": {
    "id": "t0gCXm6ncZIn"
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52aff524",
   "metadata": {
    "id": "52aff524"
   },
   "outputs": [],
   "source": [
    "def render_figure_widge(mode=None):\n",
    "    gofig = go.FigureWidget()\n",
    "    gofig.update_xaxes(title_text='Epoch', autorange=True)\n",
    "    gofig.update_yaxes(title_text='Avg. Reward', autorange=True)\n",
    "    \n",
    "    if mode == 'validation':\n",
    "        gofig.update_yaxes(title_text='Diff. Rate', autorange=True)\n",
    "        \n",
    "    gofig.update_layout(width=1200, height=800, hovermode=\"x unified\")\n",
    "    \n",
    "    # Only for question 11 and 12\n",
    "    if mode == 'Reward+Loss':\n",
    "        gofig = make_subplots(rows=1, cols=2, subplot_titles=['Average Reward', 'Training Loss'])\n",
    "        gofig.update_layout(width=1400, height=500, hovermode=\"x unified\")\n",
    "        gofig = go.FigureWidget(gofig)\n",
    "        \n",
    "    return gofig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24ef2e01",
   "metadata": {
    "id": "24ef2e01"
   },
   "outputs": [],
   "source": [
    "def init_figure_widge(fig_widge, mode, colour, epsilon_opt=None, epsilon_learn=None, n_star=None):\n",
    "    # Mode epsilon only used for question 11\n",
    "    if mode == 'epsilon':\n",
    "        if not (epsilon_learn >= 0 and epsilon_learn < 1):\n",
    "            raise Exception('Epsilon value empty or out of range')\n",
    "        \n",
    "        # For average reward - 0\n",
    "        fig_widge.add_trace(go.Scatter(x=[0], y=[0], name=f'Avg. Reward = {epsilon_learn}', marker=dict(color=colour)), row=1, col=1)\n",
    "        \n",
    "        # For training loss - 1\n",
    "        fig_widge.add_trace(go.Scatter(x=[0], y=[0], name=f'Tr. Loss = {epsilon_learn}', marker=dict(color=colour)), row=1, col=2)\n",
    "        \n",
    "    # Mode 'descrease exploration' only used for question 12\n",
    "    elif mode == 'decrease exploration':\n",
    "        if not isinstance(n_star,int):\n",
    "            raise TypeError('Cannot find or recognise n_star')       \n",
    "        \n",
    "        # For average reward - 0\n",
    "        fig_widge.add_trace(go.Scatter(x=[0], y=[0], name=f'Avg. Reward = {n_star}', marker=dict(color=colour)), row=1, col=1)\n",
    "        \n",
    "        # For training loss - 1\n",
    "        fig_widge.add_trace(go.Scatter(x=[0], y=[0], name=f'Tr. Loss = {n_star}', marker=dict(color=colour)), row=1, col=2)\n",
    "        \n",
    "    elif mode == 'validate n*':\n",
    "        fig_widge.add_scatter(x=[0], y=[0], name=f'M_opt n* = {n_star}', marker=dict(color=colour))\n",
    "        fig_widge.add_scatter(x=[0], y=[0], name=f'M_rand n* = {n_star}', marker=dict(color=colour), line=dict(dash='dot'))\n",
    "        \n",
    "    elif mode == 'validate epsilon opt':\n",
    "        fig_widge.add_scatter(x=[0], y=[0], name=f'M_opt epilson opt= {epsilon_opt}', marker=dict(color=colour))\n",
    "        fig_widge.add_scatter(x=[0], y=[0], name=f'M_rand epilson opt= {epsilon_opt}', marker=dict(color=colour), line=dict(dash='dot'))\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(\"Unable to recognise mode\")\n",
    "            \n",
    "    return fig_widge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "181888d4-32fc-4b99-bd11-7d9b2606b5c0",
   "metadata": {
    "id": "181888d4-32fc-4b99-bd11-7d9b2606b5c0"
   },
   "outputs": [],
   "source": [
    "def check_available(grid, pos):\n",
    "    \"\"\"Input grid should be 2 * 3 * 3 tensor\"\"\"\n",
    "    x, y = pos // 3, pos % 3\n",
    "    if grid[0,x,y] == grid[1,x,y] == 0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1a01956-b778-4012-a038-62f4b2f5e011",
   "metadata": {
    "id": "c1a01956-b778-4012-a038-62f4b2f5e011"
   },
   "outputs": [],
   "source": [
    "def grid2state(grid, learner_value):\n",
    "    grid2d = np.tile(grid, (2,1)).reshape(2,3,3)\n",
    "    if learner_value == \"X\":\n",
    "        grid2d[0] = np.clip(grid2d[0], 0, 1)\n",
    "        grid2d[1] = np.clip(-grid2d[1], 0, 1)\n",
    "    else:\n",
    "        grid2d[0] = np.clip(-grid2d[0], 0, 1)\n",
    "        grid2d[1] = np.clip(grid2d[1], 0, 1)\n",
    "    return torch.Tensor(grid2d).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae96edd",
   "metadata": {
    "id": "2ae96edd"
   },
   "outputs": [],
   "source": [
    "def validation(epsilon_opt, epsilon_learn, DeepQLearner, epoch=500):\n",
    "    env = TictactoeEnv()\n",
    "    Turns = np.array(['O','X'])\n",
    "    reward_sum = 0\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        np.random.seed(i)\n",
    "        \n",
    "        # Our policy makes the first move in the first 250 games\n",
    "        if i < 250:\n",
    "            Turns = np.array(['O','X'])\n",
    "        else:\n",
    "            Turns = np.array(['X','O'])\n",
    "        \n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        player_opt = OptimalPlayer(epsilon=epsilon_opt, player=Turns[0])\n",
    "        player_learn = DeepQLearner\n",
    "        player_learn.epsilon = epsilon_learn\n",
    "        player_learn.player = Turns[1]\n",
    "\n",
    "        for j in range(9):\n",
    "            if env.current_player == player_opt.player:\n",
    "                action = player_opt.act(grid)\n",
    "                grid, end, winner = env.step(action, print_grid=False)\n",
    "            else:\n",
    "                state = grid2state(grid, player_learn.player).to(DEVICE)\n",
    "                action = player_learn.act(state)\n",
    "                # new_position = player_learn.act(grid, q_library)\n",
    "                if check_available(grid2state(grid, player_learn.player), action.item()):\n",
    "                    grid, end, winner = env.step(action.item(), print_grid=False)\n",
    "                else:\n",
    "                    # End the game if the agent takes an unavailable action\n",
    "                    end = True\n",
    "                    # Give the agent a negative reward\n",
    "                    reward = -1\n",
    "                    \n",
    "                    # # Opt wins the games\n",
    "                    env.winner = player_opt.player\n",
    "\n",
    "            if end:\n",
    "                reward_sum += env.reward(player=player_learn.player)\n",
    "                env.reset()\n",
    "                break\n",
    "                \n",
    "    return reward_sum/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "z35OU4uX9eP5",
   "metadata": {
    "id": "z35OU4uX9eP5"
   },
   "outputs": [],
   "source": [
    "def run_simulation(epsilon_opt, buffer_capacity=10000, batch_size=64, epoch=20000, plot_interval=250, \n",
    "                   colour='black', epsilon_learn_init=None, n_star=None, fig_widge=None, mode=None):\n",
    "\n",
    "    any_epsilon = (epsilon_learn_init is None) and (n_star is None)\n",
    "    assert not any_epsilon, \"Both epsilon_learn and n_star is None! At least having one of them.\"\n",
    "\n",
    "    env = TictactoeEnv()        \n",
    "    Turns = np.array(['X','O'])\n",
    "    plot_interval = 250\n",
    "\n",
    "    # rendering in figure widge\n",
    "    if fig_widge is not None:\n",
    "        init_figure_widge(fig_widge, mode, colour, \n",
    "                          epsilon_opt=epsilon_opt, \n",
    "                          epsilon_learn=epsilon_learn_init, \n",
    "                          n_star=n_star)\n",
    "        \n",
    "    replay_memory = Buffer(capacity=buffer_capacity, batch_size=batch_size)\n",
    "    player_opt = OptimalPlayer(epsilon=epsilon_opt, player=Turns[0])\n",
    "    player_learn = DeepQAgent(epsilon=epsilon_learn_init, player=Turns[1])\n",
    "\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_max = 0.8\n",
    "\n",
    "    reward_sum = 0\n",
    "    loss_sum = 0\n",
    "    loss_step = 0\n",
    "    average_rewards = []\n",
    "    average_losses = []\n",
    "\n",
    "    for i in range(epoch):\n",
    "        # if mode == 'decrease exploration' or 'validation':\n",
    "        if epsilon_learn_init is None:\n",
    "            assert n_star is not None, \"In this setting, we use n_star to tune the epsilon_learn.\"\n",
    "            epsilon_learn = max(epsilon_min, epsilon_max*(1-(i+1)/n_star))\n",
    "        else:\n",
    "            epsilon_learn = epsilon_learn_init\n",
    "        \n",
    "        player_learn.epsilon = epsilon_learn\n",
    "\n",
    "        env.reset()\n",
    "        grid, _, _ = env.observe()\n",
    "\n",
    "        # Switch Order\n",
    "        Turns = Turns[::-1]\n",
    "        player_opt.player = Turns[0]\n",
    "        player_learn.player = Turns[1]\n",
    "        env.current_player  = 'X'\n",
    "\n",
    "        state = None\n",
    "        next_state = None\n",
    "\n",
    "        for j in range(9):\n",
    "            if env.current_player == player_opt.player:\n",
    "                action_opt = player_opt.act(grid)\n",
    "                grid, end, winner = env.step(action_opt, print_grid=False)\n",
    "                \n",
    "                # Get reward\n",
    "                reward = env.reward(player=player_learn.player)\n",
    "                \n",
    "                # In case opt plays first\n",
    "                if j > 0:\n",
    "                    next_state = grid2state(grid, player_learn.player)\n",
    "            else:\n",
    "                state = grid2state(grid, player_learn.player)\n",
    "                action = player_learn.act(state)\n",
    "                action = action.to(DEVICE)\n",
    "\n",
    "                # Check the availability of current action.\n",
    "                if check_available(grid2state(grid, player_learn.player), action.item()):\n",
    "                    grid, end, winner = env.step(action.item(), print_grid=False)\n",
    "\n",
    "                    # Get reward.\n",
    "                    reward = env.reward(player=player_learn.player)\n",
    "                else:\n",
    "                    # End the game if the agent takes an unavailable action\n",
    "                    end = True\n",
    "                    unavailable_action = True\n",
    "                    # Give the agent a negative reward\n",
    "                    reward = -1\n",
    "                    \n",
    "                    # Opponent wins the games\n",
    "                    env.winner = player_opt.player\n",
    "\n",
    "                    \n",
    "            if not end:\n",
    "                # In case opt players first - next_state does not exist\n",
    "                if next_state != None:\n",
    "                    replay_memory.push(state.unsqueeze(0), action, next_state, torch.tensor([reward], device=DEVICE))\n",
    "                    if len(replay_memory) >= replay_memory.batch_size:\n",
    "                        loss = player_learn.train(replay_memory)\n",
    "                        loss_step += 1\n",
    "                        loss_sum += loss\n",
    "                    next_state = None\n",
    "                    \n",
    "            if end:\n",
    "                # Once the game ends, no matter which player plays first\n",
    "                # Update is the same.\n",
    "                if env.winner == player_opt.player:\n",
    "                    # If opt wins the game, reward is guaranteed to be update-to-date\n",
    "                    reward = -1\n",
    "                elif env.winner == player_learn.player:\n",
    "                    # our agent wins.\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    # Draw\n",
    "                    reward = 0\n",
    "\n",
    "                next_state = None\n",
    "                # Update target model every 500 epoch\n",
    "                update_target = False\n",
    "\n",
    "                if (i+1) % 500 == 0:\n",
    "                    update_target = True\n",
    "                replay_memory.push(state.unsqueeze(0), action, next_state, torch.tensor([reward], device=DEVICE))\n",
    "                if len(replay_memory) >= replay_memory.batch_size:\n",
    "                    loss = player_learn.train(replay_memory, update_target)\n",
    "                    loss_step += 1\n",
    "                    loss_sum += loss\n",
    "                    \n",
    "                reward_sum += reward\n",
    "                env.reset()\n",
    "                break       \n",
    "            \n",
    "        #############################\n",
    "        ######### Plot ##############\n",
    "\n",
    "        if (i+1) % plot_interval == 0:\n",
    "            # calculate average reward at the end of the current interval.\n",
    "            average_reward = reward_sum / plot_interval  \n",
    "            average_loss = loss_sum / loss_step\n",
    "            # print(average_reward)  \n",
    "            # print(average_loss)\n",
    "            \n",
    "            if not 'validate' in mode:\n",
    "                idx = len(fig_widge.data)-2\n",
    "                \n",
    "                fig_widge.data[idx].x = np.append(fig_widge.data[idx].x, i+1)[0:]\n",
    "                fig_widge.data[idx].y = np.append(fig_widge.data[idx].y, average_reward)[0:]\n",
    "                \n",
    "                fig_widge.data[idx+1].x = np.append(fig_widge.data[idx+1].x, i+1)[0:]\n",
    "                fig_widge.data[idx+1].y = np.append(fig_widge.data[idx+1].y, average_loss.item())[0:]\n",
    "                \n",
    "            else:\n",
    "                M_opt = validation(0, 0, player_learn, epoch=500)\n",
    "                M_rand = validation(1, 0, player_learn, epoch=500)\n",
    "                \n",
    "                idx = len(fig_widge.data)-2\n",
    "                fig_widge.data[idx].x = np.append(fig_widge.data[idx].x, i+1)[0:]\n",
    "                fig_widge.data[idx].y = np.append(fig_widge.data[idx].y, M_opt)[0:]\n",
    "                fig_widge.data[idx+1].x = np.append(fig_widge.data[idx+1].x, i+1)[0:]\n",
    "                fig_widge.data[idx+1].y = np.append(fig_widge.data[idx+1].y, M_rand)[0:]\n",
    "                fig_widge.layout.title.text = f'Epoch {i+1}, M_opt = {M_opt}, M_rand = {M_rand}'\n",
    "\n",
    "            average_rewards.append(average_reward)\n",
    "            average_losses.append(average_loss)\n",
    "            # reset reward_sum.\n",
    "            reward_sum = 0\n",
    "            loss_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c0d6a0c-a019-47cf-ab1c-16e50c243df2",
   "metadata": {
    "id": "6c0d6a0c-a019-47cf-ab1c-16e50c243df2"
   },
   "outputs": [],
   "source": [
    "def self_practice_simulation(epsilon_opt=None, epoch=20000, plot_interval=250, \n",
    "                   colour='black', epsilon_learn_init=None, n_star=None, fig_widge=None, mode=None):\n",
    "\n",
    "    any_epsilon = (epsilon_learn_init is None) and (n_star is None)\n",
    "    assert not any_epsilon, \"Both epsilon_learn and n_star is None! At least having one of them.\"\n",
    "\n",
    "    env = TictactoeEnv()        \n",
    "    Turns = np.array(['X','O'])\n",
    "    plot_interval = 250\n",
    "\n",
    "    # rendering in figure widge\n",
    "    if fig_widge is not None:\n",
    "        if n_star is None:\n",
    "            fig_widge.add_scatter(x=[0], y=[0], name=f'M-opt vs. Q-agent, epsilon = {epsilon_learn_init}', marker=dict(color=colour))\n",
    "            fig_widge.add_scatter(x=[0], y=[0], name=f'M-rand vs. Q-agent, epsilon = {epsilon_learn_init}', marker=dict(color=colour), line=dict(dash='dot'))\n",
    "        else:\n",
    "            fig_widge.add_scatter(x=[0], y=[0], name=f'M-opt vs. Q-agent, n* = {n_star}', marker=dict(color=colour))\n",
    "            fig_widge.add_scatter(x=[0], y=[0], name=f'M-rand vs. Q-agent, n* = {n_star}', marker=dict(color=colour), line=dict(dash='dot'))\n",
    "        \n",
    "    replay_memory = Buffer(capacity=10000, batch_size=64)\n",
    "    shared_model = FCN()\n",
    "    # Use player_A as our agent. Need to exchange order when using player_B.\n",
    "    player_A = DeepQAgent(epsilon=epsilon_learn_init, player='O', model=shared_model)\n",
    "    player_B = DeepQAgent(epsilon=epsilon_learn_init, player='X', model=shared_model)\n",
    "\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_max = 0.8\n",
    "\n",
    "    reward_sum_A = 0\n",
    "    loss_sum_A = 0\n",
    "    loss_step_A = 0\n",
    "    average_rewards_A = []\n",
    "    average_losses_A = []\n",
    "\n",
    "    for i in range(epoch):\n",
    "        # if mode == 'decrease exploration' or 'validation':\n",
    "        if epsilon_learn_init is None:\n",
    "            assert n_star is not None, \"In this setting, we use n_star to tune the epsilon_learn.\"\n",
    "            epsilon_learn = max(epsilon_min, epsilon_max*(1-(i+1)/n_star))\n",
    "        else:\n",
    "            epsilon_learn = epsilon_learn_init\n",
    "        \n",
    "        player_A.epsilon = epsilon_learn\n",
    "        player_B.epsilon = epsilon_learn\n",
    "\n",
    "        env.reset()\n",
    "        grid, _, _ = env.observe()\n",
    "\n",
    "        # Switch Order\n",
    "        # Turns = Turns[::-1] \n",
    "        # env.current_player = Turns[0]\n",
    "        Turns = Turns[::-1]\n",
    "        player_A.player = Turns[0]\n",
    "        player_B.player = Turns[1]\n",
    "        env.current_player  = 'X'\n",
    "\n",
    "        state_A = None\n",
    "        next_state_A = None\n",
    "        state_B = None\n",
    "        next_state_B = None\n",
    "\n",
    "        for j in range(9):\n",
    "            if env.current_player == player_A.player:\n",
    "                # state_A is observed from the perspective of A.\n",
    "                state_A = grid2state(grid, player_A.player)\n",
    "                action_A = player_A.act(state_A)\n",
    "                action_A = action_A.to(DEVICE)\n",
    "                \n",
    "                # Get reward\n",
    "                if check_available(grid2state(grid, player_A.player), action_A.item()):\n",
    "                    grid, end, winner = env.step(action_A.item(), print_grid=False)\n",
    "\n",
    "                    # Get reward.\n",
    "                    reward_A = env.reward(player=player_A.player)\n",
    "                    # In case A plays first.\n",
    "                    if j > 0:\n",
    "                        next_state_B = grid2state(grid, player_B.player)\n",
    "                else:\n",
    "                    # End the game if the agent takes an unavailable action\n",
    "                    end = True\n",
    "                    # Give the agent a negative reward\n",
    "                    reward_A = -1\n",
    "                    \n",
    "                    # # Opt wins the games\n",
    "                    env.winner = player_B.player\n",
    "\n",
    "            else:\n",
    "                # state_B is observed from the perspective of B.\n",
    "                state_B = grid2state(grid, player_B.player)\n",
    "                action_B = player_B.act(state_B)\n",
    "                action_B = action_B.to(DEVICE)\n",
    "\n",
    "                if check_available(grid2state(grid, player_B.player), action_B.item()):\n",
    "                    grid, end, winner = env.step(action_B.item(), print_grid=False)\n",
    "\n",
    "                    # Get reward.\n",
    "                    reward_B = env.reward(player=player_B.player)\n",
    "                    # In case A plays first\n",
    "                    if j > 0:\n",
    "                        next_state_A = grid2state(grid, player_A.player)\n",
    "                else:\n",
    "                    # End the game if the agent takes an unavailable action\n",
    "                    end = True\n",
    "                    # Give the agent a negative reward\n",
    "                    reward_B = -1\n",
    "                    \n",
    "                    # # Opt wins the games\n",
    "                    env.winner = player_A.player\n",
    "    \n",
    "            if not end:\n",
    "                # In case opt players first - next_state does not exist\n",
    "                if next_state_A != None:\n",
    "                    replay_memory.push(state_A.unsqueeze(0), action_A, next_state_A, torch.tensor([reward_A], device=DEVICE))\n",
    "                    if len(replay_memory) >= replay_memory.batch_size:\n",
    "                        loss_A = player_A.train(replay_memory)\n",
    "                        loss_sum_A += loss_A\n",
    "                        loss_step_A += 1\n",
    "                    next_state_A = None\n",
    "                \n",
    "                if next_state_B != None:\n",
    "                    replay_memory.push(state_B.unsqueeze(0), action_B, next_state_B, torch.tensor([reward_B], device=DEVICE))\n",
    "                    if len(replay_memory) >= replay_memory.batch_size:\n",
    "                        _ = player_B.train(replay_memory)\n",
    "                    next_state_B = None\n",
    "                          \n",
    "            if end:\n",
    "                # Once ending the game, no matter which player plays first\n",
    "                # Update is the same\n",
    "                if env.winner == player_A.player:\n",
    "                    # If opt wins the game, reward is guaranteed to be update-to-date\n",
    "                    reward_A = 1\n",
    "                    reward_B = -1\n",
    "                elif env.winner == player_B.player:\n",
    "                    reward_A = -1\n",
    "                    reward_B = 1\n",
    "                else:\n",
    "                    # Draw\n",
    "                    reward_A = 0\n",
    "                    reward_B = 0\n",
    "                \n",
    "                # If one agent wins the game, we don't actually real next_state\n",
    "                # Because q(s', a') will be cancelled out.\n",
    "                next_state_A = None\n",
    "                next_state_B = None\n",
    "                \n",
    "                replay_memory.push(state_A.unsqueeze(0), action_A, next_state_A, torch.tensor([reward_A], device=DEVICE))\n",
    "                replay_memory.push(state_B.unsqueeze(0), action_B, next_state_B, torch.tensor([reward_B], device=DEVICE))\n",
    "                \n",
    "                # Update target model every 500 epoch.\n",
    "                update_target = False\n",
    "\n",
    "                if (i+1) % 500 == 0:\n",
    "                    update_target = True\n",
    "\n",
    "                # Training model when game over, and replay_memory size >= batch size\n",
    "                if len(replay_memory) >= replay_memory.batch_size:\n",
    "                    loss_A = player_A.train(replay_memory, update_target)\n",
    "                    loss_sum_A += loss_A\n",
    "                    loss_step_A += 1\n",
    "                    _ = player_B.train(replay_memory, update_target)\n",
    "\n",
    "                    \n",
    "                # always focus on the average reward of player_A.\n",
    "                reward_sum_A += reward_A\n",
    "                env.reset()\n",
    "                break       \n",
    "            \n",
    "        #############################\n",
    "        ######### Plot ##############\n",
    "\n",
    "        if (i+1) % plot_interval == 0:\n",
    "            # calculate average reward at the end of the current interval.\n",
    "            average_reward_A = reward_sum_A / plot_interval  \n",
    "            average_loss_A = loss_sum_A / loss_step_A\n",
    "            print(average_reward_A)  \n",
    "            print(average_loss_A)\n",
    "            \n",
    "            M_opt_1 = validation(0, 0, player_A, epoch=500)\n",
    "            M_rand_1 = validation(1, 0, player_A, epoch=500)\n",
    "\n",
    "            idx = len(fig_widge.data)-2\n",
    "            fig_widge.data[idx].x = np.append(fig_widge.data[idx].x, i+1)[0:]\n",
    "            fig_widge.data[idx].y = np.append(fig_widge.data[idx].y, M_opt_1)[0:]\n",
    "            fig_widge.data[idx+1].x = np.append(fig_widge.data[idx+1].x, i+1)[0:]\n",
    "            fig_widge.data[idx+1].y = np.append(fig_widge.data[idx+1].y, M_rand_1)[0:]\n",
    "            fig_widge.layout.title.text = f'Epoch {i+1}, M_opt = {M_opt_1}, M_rand = {M_rand_1}'\n",
    "\n",
    "            average_rewards_A.append(average_reward_A)\n",
    "            average_losses_A.append(average_loss_A)\n",
    "            # reset reward_sum.\n",
    "            reward_sum_A = 0\n",
    "            loss_sum_A = 0\n",
    "            loss_step_A = 0\n",
    "\n",
    "    return (shared_model, replay_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9AvVt2EdjrUK",
   "metadata": {
    "id": "9AvVt2EdjrUK"
   },
   "outputs": [],
   "source": [
    "def generate_text(lst):\n",
    "    text = ['X' if x == 1 else 'O' if x == -1 else '' for x in lst]\n",
    "    return np.flip(np.reshape(text, (3, 3)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "KsrEGmDYjryG",
   "metadata": {
    "id": "KsrEGmDYjryG"
   },
   "outputs": [],
   "source": [
    "def retrieve_avail_qv(q_values, states):\n",
    "    return [val if (states[idx] == 0) else 0 for (idx, val) in enumerate(q_values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e41f0e6",
   "metadata": {
    "id": "0e41f0e6"
   },
   "source": [
    "Question 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe589738",
   "metadata": {
    "id": "fe589738"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'DOMWidget' has no attribute '_ipython_display_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/rl/env/lib/python3.9/site-packages/IPython/core/formatters.py:921\u001b[0m, in \u001b[0;36mIPythonDisplayFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    919\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 921\u001b[0m     \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/rl/env/lib/python3.9/site-packages/plotly/basewidget.py:741\u001b[0m, in \u001b[0;36mBaseFigureWidget._ipython_display_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;124;03mHandle rich display of figures in ipython contexts\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# Override BaseFigure's display to make sure we display the widget version\u001b[39;00m\n\u001b[0;32m--> 741\u001b[0m \u001b[43mwidgets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDOMWidget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ipython_display_\u001b[49m(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'DOMWidget' has no attribute '_ipython_display_'"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Average Reward",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Training Loss",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 500,
        "hovermode": "x unified",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 1400,
        "xaxis": {
         "anchor": "y",
         "autorange": true,
         "domain": [
          0,
          0.45
         ],
         "range": [
          -1,
          6
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "autorange": true,
         "domain": [
          0,
          1
         ],
         "range": [
          -1,
          4
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ]
        }
       }
      },
      "image/png": "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",
      "text/html": [
       "<div>                            <div id=\"e1f8e8a4-c768-4366-88ae-ba5a8d937ea7\" class=\"plotly-graph-div\" style=\"height:500px; width:1400px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e1f8e8a4-c768-4366-88ae-ba5a8d937ea7\")) {                    Plotly.newPlot(                        \"e1f8e8a4-c768-4366-88ae-ba5a8d937ea7\",                        [],                        {\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Average Reward\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Training Loss\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"height\":500,\"hovermode\":\"x unified\",\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"width\":1400,\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0]}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('e1f8e8a4-c768-4366-88ae-ba5a8d937ea7');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ],
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [],\n",
       "    'layout': {'annotations': [{'font': {'size': 16},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'Average Reward',\n",
       "                                'x': 0.225,\n",
       "                                'xanchor': 'center',\n",
       "                                'xref': 'paper',\n",
       "                                'y': 1.0,\n",
       "                                'yanchor': 'bottom',\n",
       "                                'yref': 'paper'},\n",
       "                               {'font': {'size': 16},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'Training Loss',\n",
       "                                'x': 0.775,\n",
       "                                'xanchor': 'center',\n",
       "                                'xref': 'paper',\n",
       "                                'y': 1.0,\n",
       "                                'yanchor': 'bottom',\n",
       "                                'yref': 'paper'}],\n",
       "               'height': 500,\n",
       "               'hovermode': 'x unified',\n",
       "               'template': '...',\n",
       "               'width': 1400,\n",
       "               'xaxis': {'anchor': 'y', 'domain': [0.0, 0.45]},\n",
       "               'xaxis2': {'anchor': 'y2', 'domain': [0.55, 1.0]},\n",
       "               'yaxis': {'anchor': 'x', 'domain': [0.0, 1.0]},\n",
       "               'yaxis2': {'anchor': 'x2', 'domain': [0.0, 1.0]}}\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw1 = render_figure_widge('Reward+Loss')\n",
    "fw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7f1087b-dcb9-4927-a855-10dc7d75c32d",
   "metadata": {
    "id": "d7f1087b-dcb9-4927-a855-10dc7d75c32d"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking arugment for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m plot_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m  \u001b[38;5;66;03m# The interval of games to calculate an average reward.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, epsilon \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epsilon_learn):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepsilon_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon_opt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepsilon_learn_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfig_widge\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolour\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolours\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepsilon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [11], line 66\u001b[0m, in \u001b[0;36mrun_simulation\u001b[0;34m(epsilon_opt, buffer_capacity, batch_size, epoch, plot_interval, colour, epsilon_learn_init, n_star, fig_widge, mode)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     state \u001b[38;5;241m=\u001b[39m grid2state(grid, player_learn\u001b[38;5;241m.\u001b[39mplayer)\n\u001b[0;32m---> 66\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mplayer_learn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Check the availability of current action.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [3], line 554\u001b[0m, in \u001b[0;36mDeepQAgent.act\u001b[0;34m(self, grid)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 554\u001b[0m         action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    555\u001b[0m         _, action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(action_probs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[0;32m~/rl/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [3], line 501\u001b[0m, in \u001b[0;36mFCN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    500\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 501\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m    503\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m~/rl/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/rl/env/lib/python3.9/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rl/env/lib/python3.9/site-packages/torch/nn/functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight):\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1847\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking arugment for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "epsilon_opt = 0.5  # This epsilon value represents exploration level of optimal player. Fixed in Question 1.\n",
    "epsilon_learn = [0.0, 0.2, 0.4, 0.8]  # This epsilon value represents exploration level of Q-learning agent. Defind by ourselves.\n",
    "epoch = 20000  # number of games to play in Question 1.\n",
    "plot_interval = 250  # The interval of games to calculate an average reward.\n",
    "for idx, epsilon in enumerate(epsilon_learn):\n",
    "    run_simulation(\n",
    "        epsilon_opt=epsilon_opt,\n",
    "        epsilon_learn_init=epsilon, \n",
    "        fig_widge=fw1,\n",
    "        colour=colours[idx],\n",
    "        mode='epsilon'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdd4c93",
   "metadata": {
    "id": "0bdd4c93"
   },
   "source": [
    "Question 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f5fc5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537,
     "referenced_widgets": [
      "66eb89e1c0104a94ad594f6ba2f36fce"
     ]
    },
    "id": "df3f5fc5",
    "outputId": "76862377-7306-4114-ce14-8e6bd9752f39"
   },
   "outputs": [],
   "source": [
    "fw2 = render_figure_widge('Reward+Loss')\n",
    "fw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7266b52d",
   "metadata": {
    "id": "7266b52d"
   },
   "outputs": [],
   "source": [
    "epsilon_opt = 0.5  # This epsilon value represents exploration level of optimal player. Fixed in Question 1.\n",
    "epsilon_learn = [0.0, 0.2, 0.4, 0.8]  # This epsilon value represents exploration level of Q-learning agent. Defind by ourselves.\n",
    "epoch = 20000  # number of games to play in Question 1.\n",
    "plot_interval = 250  # The interval of games to calculate an average reward.\n",
    "for idx, epsilon in enumerate(epsilon_learn):\n",
    "    run_simulation(\n",
    "        epsilon_opt=epsilon_opt,\n",
    "        buffer_capacity=1,\n",
    "        batch_size=1,\n",
    "        epsilon_learn_init=epsilon, \n",
    "        fig_widge=fw2,\n",
    "        colour=colours[idx],\n",
    "        mode='epsilon'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf023286",
   "metadata": {
    "id": "cf023286"
   },
   "source": [
    "Question 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529107b6-0fa6-4c7e-96a3-fb323311bf0a",
   "metadata": {
    "id": "529107b6-0fa6-4c7e-96a3-fb323311bf0a"
   },
   "outputs": [],
   "source": [
    "fw3 = render_figure_widge()\n",
    "fw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeebfa18-d036-4643-a683-63d4f884c859",
   "metadata": {
    "id": "aeebfa18-d036-4643-a683-63d4f884c859"
   },
   "outputs": [],
   "source": [
    "epsilon_opt = 0.5  # This epsilon value represents exploration level of optimal player.\n",
    "n_stars = [1, 100, 1000, 2000, 5000, 10000, 20000, 40000]\n",
    "epoch = 20000  # number of games to play in Question 1.\n",
    "plot_interval = 250  # The interval of games to calculate an average reward.\n",
    "for idx, n_s in enumerate(n_stars):\n",
    "    run_simulation(\n",
    "        epsilon_opt=epsilon_opt, \n",
    "        n_star=n_s,\n",
    "        epoch=epoch, \n",
    "        plot_interval=plot_interval, \n",
    "        colour=colours[idx%len(colours)], \n",
    "        fig_widge=fw3, \n",
    "        mode=\"validate n*\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AJyy2S_nE18U",
   "metadata": {
    "id": "AJyy2S_nE18U"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3VAe8vK0E18Z",
   "metadata": {
    "id": "3VAe8vK0E18Z"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.disable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ke-JVjUOoDJ9",
   "metadata": {
    "id": "ke-JVjUOoDJ9"
   },
   "source": [
    "Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39MJL2IoFML",
   "metadata": {
    "id": "f39MJL2IoFML"
   },
   "outputs": [],
   "source": [
    "fw4 = render_figure_widge()\n",
    "fw4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7nAZlZiAoIMe",
   "metadata": {
    "id": "7nAZlZiAoIMe"
   },
   "outputs": [],
   "source": [
    "epsilon_opts = [.0, .2, .4, .6, .8]\n",
    "best_n_star = 1000\n",
    "for idx, epsilon_opt in enumerate(epsilon_opts):\n",
    "    run_simulation(\n",
    "        epsilon_opt=epsilon_opt, \n",
    "        n_star=best_n_star,\n",
    "        fig_widge=fw4,\n",
    "        colour=colours[idx%len(colours)],\n",
    "        mode='validate epsilon opt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fOKfQ2op4k",
   "metadata": {
    "id": "c7fOKfQ2op4k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "pQ_nLUQwnZuD",
   "metadata": {
    "id": "pQ_nLUQwnZuD"
   },
   "source": [
    "Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NsjlRmABnbWb",
   "metadata": {
    "id": "NsjlRmABnbWb"
   },
   "outputs": [],
   "source": [
    "fw5 = render_figure_widge('validation')\n",
    "fw5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZthOL_HLne4N",
   "metadata": {
    "id": "ZthOL_HLne4N"
   },
   "outputs": [],
   "source": [
    "epsilon_learn = [0, 0.2, 0.4, 0.6, 0.8]\n",
    "for idx, epsilon in enumerate(epsilon_learn):\n",
    "    self_practice_simulation(\n",
    "        colour=colours[idx%len(colours)],\n",
    "        epsilon_learn_init=epsilon,\n",
    "        fig_widge=fw5,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXjgfuq0nj8s",
   "metadata": {
    "id": "xXjgfuq0nj8s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Nixtwuw8oqG6",
   "metadata": {
    "id": "Nixtwuw8oqG6"
   },
   "source": [
    "Question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T0aGwttdo3Gq",
   "metadata": {
    "id": "T0aGwttdo3Gq"
   },
   "outputs": [],
   "source": [
    "fw6 = render_figure_widge('validation')\n",
    "fw6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fRIJL6Cto4rm",
   "metadata": {
    "id": "fRIJL6Cto4rm"
   },
   "outputs": [],
   "source": [
    "n_stars = [1, 100, 1000, 5000, 10000, 20000]\n",
    "libraries = []\n",
    "\n",
    "for idx, n_s in enumerate(n_stars):\n",
    "    libraries.append(self_practice_simulation(colour=colours[idx%len(colours)],n_star=n_s,fig_widge=fw6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cJGQUOO6jhe_",
   "metadata": {
    "id": "cJGQUOO6jhe_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Xa0Wy7C-jh_B",
   "metadata": {
    "id": "Xa0Wy7C-jh_B"
   },
   "source": [
    "Question 19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JLnzJ5pokKBL",
   "metadata": {
    "id": "JLnzJ5pokKBL"
   },
   "outputs": [],
   "source": [
    "# Retrieve FCN model for qvalue prediction and memory for sampling\n",
    "model, memory = self_practice_simulation(colour='black',n_star=1000,fig_widge=render_figure_widge('validation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q5Ygphb1jj07",
   "metadata": {
    "id": "Q5Ygphb1jj07"
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=3)\n",
    "\n",
    "for i in range(3):\n",
    "    random_sample = memory.sample(1)\n",
    "    state  = random_sample[0].state\n",
    "    grid = state[0][0] - state[0][1]\n",
    "    lst = grid_to_state(grid.cpu().detach().numpy())\n",
    "\n",
    "    qvalue = model(state)[0].cpu().tolist()\n",
    "    \n",
    "    fig.add_trace(go.Heatmap(z=np.flip(np.reshape(retrieve_avail_qv(qvalue, lst), (3,3)), 0),\n",
    "                         text= generate_text(lst), texttemplate=\"%{text}\",\n",
    "                         textfont={\"size\":20}, coloraxis='coloraxis'), row=1, col=i+1)\n",
    "    \n",
    "fig.update_layout(width=1200, height=500)\n",
    "fig.update_xaxes(visible=False)       \n",
    "fig.update_yaxes(visible=False)\n",
    "fig.update_layout(coloraxis=dict(colorscale='RdBu', colorbar_thickness=23))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Deep_Q_full_better_comment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "65a2bc8204bd30e294b2f94e2545ee4e57c8e2de41c09c84b6b285528f35f0a1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "66eb89e1c0104a94ad594f6ba2f36fce": {
     "model_module": "jupyterlab-plotly",
     "model_module_version": "^5.5.0",
     "model_name": "FigureModel",
     "state": {
      "_config": {
       "plotlyServerURL": "https://plot.ly"
      },
      "_data": [],
      "_dom_classes": [],
      "_js2py_layoutDelta": null,
      "_js2py_pointsCallback": null,
      "_js2py_relayout": {},
      "_js2py_restyle": {},
      "_js2py_traceDeltas": null,
      "_js2py_update": {},
      "_last_layout_edit_id": 1288,
      "_last_trace_edit_id": 1288,
      "_layout": {
       "annotations": [
        {
         "font": {
          "size": 16
         },
         "showarrow": false,
         "text": "Average Reward",
         "x": 0.225,
         "xanchor": "center",
         "xref": "paper",
         "y": 1,
         "yanchor": "bottom",
         "yref": "paper"
        },
        {
         "font": {
          "size": 16
         },
         "showarrow": false,
         "text": "Training Loss",
         "x": 0.775,
         "xanchor": "center",
         "xref": "paper",
         "y": 1,
         "yanchor": "bottom",
         "yref": "paper"
        }
       ],
       "height": 500,
       "hovermode": "x unified",
       "template": {
        "data": {
         "bar": [
          {
           "error_x": {
            "color": "#2a3f5f"
           },
           "error_y": {
            "color": "#2a3f5f"
           },
           "marker": {
            "line": {
             "color": "#E5ECF6",
             "width": 0.5
            },
            "pattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            }
           },
           "type": "bar"
          }
         ],
         "barpolar": [
          {
           "marker": {
            "line": {
             "color": "#E5ECF6",
             "width": 0.5
            },
            "pattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            }
           },
           "type": "barpolar"
          }
         ],
         "carpet": [
          {
           "aaxis": {
            "endlinecolor": "#2a3f5f",
            "gridcolor": "white",
            "linecolor": "white",
            "minorgridcolor": "white",
            "startlinecolor": "#2a3f5f"
           },
           "baxis": {
            "endlinecolor": "#2a3f5f",
            "gridcolor": "white",
            "linecolor": "white",
            "minorgridcolor": "white",
            "startlinecolor": "#2a3f5f"
           },
           "type": "carpet"
          }
         ],
         "choropleth": [
          {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           },
           "type": "choropleth"
          }
         ],
         "contour": [
          {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           },
           "colorscale": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "type": "contour"
          }
         ],
         "contourcarpet": [
          {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           },
           "type": "contourcarpet"
          }
         ],
         "heatmap": [
          {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           },
           "colorscale": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "type": "heatmap"
          }
         ],
         "heatmapgl": [
          {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           },
           "colorscale": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "type": "heatmapgl"
          }
         ],
         "histogram": [
          {
           "marker": {
            "pattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            }
           },
           "type": "histogram"
          }
         ],
         "histogram2d": [
          {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           },
           "colorscale": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "type": "histogram2d"
          }
         ],
         "histogram2dcontour": [
          {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           },
           "colorscale": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "type": "histogram2dcontour"
          }
         ],
         "mesh3d": [
          {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           },
           "type": "mesh3d"
          }
         ],
         "parcoords": [
          {
           "line": {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           },
           "type": "parcoords"
          }
         ],
         "pie": [
          {
           "automargin": true,
           "type": "pie"
          }
         ],
         "scatter": [
          {
           "marker": {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           },
           "type": "scatter"
          }
         ],
         "scatter3d": [
          {
           "line": {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           },
           "marker": {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           },
           "type": "scatter3d"
          }
         ],
         "scattercarpet": [
          {
           "marker": {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           },
           "type": "scattercarpet"
          }
         ],
         "scattergeo": [
          {
           "marker": {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           },
           "type": "scattergeo"
          }
         ],
         "scattergl": [
          {
           "marker": {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           },
           "type": "scattergl"
          }
         ],
         "scattermapbox": [
          {
           "marker": {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           },
           "type": "scattermapbox"
          }
         ],
         "scatterpolar": [
          {
           "marker": {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           },
           "type": "scatterpolar"
          }
         ],
         "scatterpolargl": [
          {
           "marker": {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           },
           "type": "scatterpolargl"
          }
         ],
         "scatterternary": [
          {
           "marker": {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           },
           "type": "scatterternary"
          }
         ],
         "surface": [
          {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           },
           "colorscale": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "type": "surface"
          }
         ],
         "table": [
          {
           "cells": {
            "fill": {
             "color": "#EBF0F8"
            },
            "line": {
             "color": "white"
            }
           },
           "header": {
            "fill": {
             "color": "#C8D4E3"
            },
            "line": {
             "color": "white"
            }
           },
           "type": "table"
          }
         ]
        },
        "layout": {
         "annotationdefaults": {
          "arrowcolor": "#2a3f5f",
          "arrowhead": 0,
          "arrowwidth": 1
         },
         "autotypenumbers": "strict",
         "coloraxis": {
          "colorbar": {
           "outlinewidth": 0,
           "ticks": ""
          }
         },
         "colorscale": {
          "diverging": [
           [
            0,
            "#8e0152"
           ],
           [
            0.1,
            "#c51b7d"
           ],
           [
            0.2,
            "#de77ae"
           ],
           [
            0.3,
            "#f1b6da"
           ],
           [
            0.4,
            "#fde0ef"
           ],
           [
            0.5,
            "#f7f7f7"
           ],
           [
            0.6,
            "#e6f5d0"
           ],
           [
            0.7,
            "#b8e186"
           ],
           [
            0.8,
            "#7fbc41"
           ],
           [
            0.9,
            "#4d9221"
           ],
           [
            1,
            "#276419"
           ]
          ],
          "sequential": [
           [
            0,
            "#0d0887"
           ],
           [
            0.1111111111111111,
            "#46039f"
           ],
           [
            0.2222222222222222,
            "#7201a8"
           ],
           [
            0.3333333333333333,
            "#9c179e"
           ],
           [
            0.4444444444444444,
            "#bd3786"
           ],
           [
            0.5555555555555556,
            "#d8576b"
           ],
           [
            0.6666666666666666,
            "#ed7953"
           ],
           [
            0.7777777777777778,
            "#fb9f3a"
           ],
           [
            0.8888888888888888,
            "#fdca26"
           ],
           [
            1,
            "#f0f921"
           ]
          ],
          "sequentialminus": [
           [
            0,
            "#0d0887"
           ],
           [
            0.1111111111111111,
            "#46039f"
           ],
           [
            0.2222222222222222,
            "#7201a8"
           ],
           [
            0.3333333333333333,
            "#9c179e"
           ],
           [
            0.4444444444444444,
            "#bd3786"
           ],
           [
            0.5555555555555556,
            "#d8576b"
           ],
           [
            0.6666666666666666,
            "#ed7953"
           ],
           [
            0.7777777777777778,
            "#fb9f3a"
           ],
           [
            0.8888888888888888,
            "#fdca26"
           ],
           [
            1,
            "#f0f921"
           ]
          ]
         },
         "colorway": [
          "#636efa",
          "#EF553B",
          "#00cc96",
          "#ab63fa",
          "#FFA15A",
          "#19d3f3",
          "#FF6692",
          "#B6E880",
          "#FF97FF",
          "#FECB52"
         ],
         "font": {
          "color": "#2a3f5f"
         },
         "geo": {
          "bgcolor": "white",
          "lakecolor": "white",
          "landcolor": "#E5ECF6",
          "showlakes": true,
          "showland": true,
          "subunitcolor": "white"
         },
         "hoverlabel": {
          "align": "left"
         },
         "hovermode": "closest",
         "mapbox": {
          "style": "light"
         },
         "paper_bgcolor": "white",
         "plot_bgcolor": "#E5ECF6",
         "polar": {
          "angularaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": ""
          },
          "bgcolor": "#E5ECF6",
          "radialaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": ""
          }
         },
         "scene": {
          "xaxis": {
           "backgroundcolor": "#E5ECF6",
           "gridcolor": "white",
           "gridwidth": 2,
           "linecolor": "white",
           "showbackground": true,
           "ticks": "",
           "zerolinecolor": "white"
          },
          "yaxis": {
           "backgroundcolor": "#E5ECF6",
           "gridcolor": "white",
           "gridwidth": 2,
           "linecolor": "white",
           "showbackground": true,
           "ticks": "",
           "zerolinecolor": "white"
          },
          "zaxis": {
           "backgroundcolor": "#E5ECF6",
           "gridcolor": "white",
           "gridwidth": 2,
           "linecolor": "white",
           "showbackground": true,
           "ticks": "",
           "zerolinecolor": "white"
          }
         },
         "shapedefaults": {
          "line": {
           "color": "#2a3f5f"
          }
         },
         "ternary": {
          "aaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": ""
          },
          "baxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": ""
          },
          "bgcolor": "#E5ECF6",
          "caxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": ""
          }
         },
         "title": {
          "x": 0.05
         },
         "xaxis": {
          "automargin": true,
          "gridcolor": "white",
          "linecolor": "white",
          "ticks": "",
          "title": {
           "standoff": 15
          },
          "zerolinecolor": "white",
          "zerolinewidth": 2
         },
         "yaxis": {
          "automargin": true,
          "gridcolor": "white",
          "linecolor": "white",
          "ticks": "",
          "title": {
           "standoff": 15
          },
          "zerolinecolor": "white",
          "zerolinewidth": 2
         }
        }
       },
       "width": 1400,
       "xaxis": {
        "anchor": "y",
        "domain": [
         0,
         0.45
        ]
       },
       "xaxis2": {
        "anchor": "y2",
        "domain": [
         0.55,
         1
        ]
       },
       "yaxis": {
        "anchor": "x",
        "domain": [
         0,
         1
        ]
       },
       "yaxis2": {
        "anchor": "x2",
        "domain": [
         0,
         1
        ]
       }
      },
      "_model_module": "jupyterlab-plotly",
      "_model_module_version": "^5.5.0",
      "_model_name": "FigureModel",
      "_py2js_addTraces": null,
      "_py2js_animate": {},
      "_py2js_deleteTraces": {},
      "_py2js_moveTraces": {},
      "_py2js_relayout": {},
      "_py2js_removeLayoutProps": {},
      "_py2js_removeTraceProps": {},
      "_py2js_restyle": null,
      "_py2js_update": {},
      "_view_count": 1,
      "_view_module": "jupyterlab-plotly",
      "_view_module_version": "^5.5.0",
      "_view_name": "FigureView"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
